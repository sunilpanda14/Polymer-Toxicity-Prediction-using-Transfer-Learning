{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea10d9fa",
   "metadata": {},
   "source": [
    "# Fine-tuned Model with Frozen Layers with 100% Ploymer data Fine tuned on pretrained Molecule model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5830597f",
   "metadata": {},
   "source": [
    "##  GPU Avaiablity Chcek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4336fb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import torch\n",
    "\n",
    "def get_filtered_free_gpus(allowed_gpus={0, 2, 3}):\n",
    "    \"\"\"Return (gpu_id, free_mem_MB) from allowed_gpus, sorted by free memory.\"\"\"\n",
    "    try:\n",
    "        result = subprocess.check_output(\n",
    "            ['nvidia-smi', '--query-gpu=index,memory.free', '--format=csv,noheader,nounits'],\n",
    "            encoding='utf-8'\n",
    "        )\n",
    "        gpu_info = []\n",
    "        for line in result.strip().split('\\n'):\n",
    "            gpu_id, mem_free = line.strip().split(',')\n",
    "            gpu_id = int(gpu_id)\n",
    "            mem_free = int(mem_free)\n",
    "            if gpu_id in allowed_gpus:\n",
    "                gpu_info.append((gpu_id, mem_free))\n",
    "        return sorted(gpu_info, key=lambda x: x[1], reverse=True)\n",
    "    except Exception as e:\n",
    "        print(\"‚ö†Ô∏è Could not query GPU info:\", e)\n",
    "        return []\n",
    "\n",
    "# Step 1: Get preferred GPU (among 0, 2, 3)\n",
    "available_gpus = get_filtered_free_gpus()\n",
    "\n",
    "if available_gpus:\n",
    "    selected_system_gpu = available_gpus[0][0]\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(selected_system_gpu)\n",
    "    print(f\"üß† Selected system GPU ID: {selected_system_gpu} with {available_gpus[0][1]} MB free memory.\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No preferred GPUs available. Using CPU.\")\n",
    "\n",
    "# Step 2: PyTorch setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    current_device = torch.cuda.current_device()\n",
    "    visible_device_name = torch.cuda.get_device_name(current_device)\n",
    "    visible_ids = os.environ[\"CUDA_VISIBLE_DEVICES\"].split(\",\")\n",
    "    real_system_gpu_id = visible_ids[current_device]\n",
    "\n",
    "    print(\"‚úÖ GPU is available!\")\n",
    "    print(f\"üñ•Ô∏è Visible PyTorch Device: cuda:{current_device}\")\n",
    "    print(f\"üß≠ Actual system GPU ID: {real_system_gpu_id}\")\n",
    "    print(f\"üìü GPU Name: {visible_device_name}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è GPU not available. Using CPU.\")\n",
    "\n",
    "print(f\"Model will run on: {device}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e551d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(\"My PID:\", os.getpid())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed707abf",
   "metadata": {},
   "source": [
    "## 1. # IMPORTS AND GLOBAL CONFIGURATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b008508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, precision_score, recall_score,\n",
    "    roc_auc_score, confusion_matrix, classification_report\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import ast\n",
    "from tqdm.notebook import tqdm\n",
    "import warnings\n",
    "import copy\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "\n",
    "print(\"Libraries imported and random seed set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0b6ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CONFIGURATION AND PATHS FOR FEW-SHOT ---\n",
    "BASE_MODEL_PATH = \"/home/sunil/am2/poetry-demo/Polytox_Matser_Thesis/Few_shot_learning_with_validation_data_Monomers/Frozen_layer_96_DNN_Model_2/models/base_model.pt\"\n",
    "PARAMS_PATH = \"/home/sunil/am2/poetry-demo/Polytox_Matser_Thesis/Few_shot_learning_with_validation_data_Monomers/Frozen_layer_96_DNN_Model_2/optuna_results/best_params.json\"\n",
    "POLYMER_DATA_PATH = \"/home/sunil/am2/poetry-demo/Polytox_Matser_Thesis/Few_shot_learning_with_validation_data_Monomers/Frozen_layer_96_DNN_Model_2/polymer_DNN_encoded_final_data.csv\"\n",
    "\n",
    "# Directory to save new results\n",
    "RESULTS_DIR = \"./results\"\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "# Define class names for plotting and reports\n",
    "class_names = ['Not_Fulfilled', 'Fulfilled', 'Under_Investigation']\n",
    "\n",
    "print(\"Configuration for Few-Shot evaluation is set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c72576",
   "metadata": {},
   "source": [
    "## 2. DEFINE YOUR DNN MODEL CLASS(ToxiciityDNN CLASS DEFINITION )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d231705f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ToxicityDNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, dropout_rates, activation_names, activation_params_list):\n",
    "        super(ToxicityDNN, self).__init__()\n",
    "        self.n_layers = len(hidden_dims)\n",
    "        self.dropout_rates = dropout_rates\n",
    "        self.activation_names = activation_names # Store for potential later inspection if needed\n",
    "        self.activation_params_list = activation_params_list # Store for potential later inspection if needed\n",
    "\n",
    "        self.activation_modules = nn.ModuleList() # Use ModuleList for activations\n",
    "\n",
    "        # Combine Linear and Dropout layers into a single ModuleList for simpler iteration\n",
    "        self.linear_layers = nn.ModuleList()\n",
    "        self.dropout_layers = nn.ModuleList()\n",
    "\n",
    "        in_dim = input_dim\n",
    "\n",
    "        for i in range(self.n_layers):\n",
    "            out_dim = hidden_dims[i]\n",
    "            self.linear_layers.append(nn.Linear(in_dim, out_dim)) # Append linear layer\n",
    "\n",
    "            # Create activation module for the current layer\n",
    "            activation_name = activation_names[i]\n",
    "            # Safely get params for the current layer, default to empty dict if index out of bounds\n",
    "            activation_params = activation_params_list[i] if i < len(activation_params_list) else {}\n",
    "\n",
    "            if activation_name == 'ReLU':\n",
    "                self.activation_modules.append(nn.ReLU())\n",
    "            elif activation_name == 'PReLU':\n",
    "                # num_parameters should match the input feature dimension if you want one param per feature\n",
    "                # or just 1 if shared across all features. For simplicity, we'll keep 1 as in your original\n",
    "                self.activation_modules.append(nn.PReLU(num_parameters=1, init=activation_params.get('init', 0.25)))\n",
    "            elif activation_name == 'SiLU':\n",
    "                self.activation_modules.append(nn.SiLU())\n",
    "            elif activation_name == 'LeakyReLU':\n",
    "                self.activation_modules.append(nn.LeakyReLU(negative_slope=activation_params.get('negative_slope', 0.01)))\n",
    "            elif activation_name == 'ELU':\n",
    "                self.activation_modules.append(nn.ELU(alpha=activation_params.get('alpha', 1.0)))\n",
    "            elif activation_name == 'GELU':\n",
    "                self.activation_modules.append(nn.GELU())\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown activation function: {activation_name}\")\n",
    "\n",
    "            self.dropout_layers.append(nn.Dropout(dropout_rates[i])) # Append dropout layer\n",
    "            in_dim = out_dim\n",
    "\n",
    "        # Output layer\n",
    "        self.output_layer = nn.Linear(in_dim, 3) # Assuming 3 output classes\n",
    "\n",
    "    # ONLY ONE FORWARD METHOD\n",
    "    def forward(self, x):\n",
    "        for i in range(self.n_layers):\n",
    "            x = self.linear_layers[i](x)      # Apply Linear layer\n",
    "            x = self.activation_modules[i](x) # Apply specific activation for this layer\n",
    "            x = self.dropout_layers[i](x)     # Apply Dropout layer\n",
    "\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "    # Remove or modify get_activation_info if you need it to reflect per-layer info\n",
    "    # For example, to get info for all layers:\n",
    "    def get_all_activation_info(self):\n",
    "        \"\"\"\n",
    "        Get information about activation functions used in all layers.\n",
    "        \"\"\"\n",
    "        info = []\n",
    "        for i in range(self.n_layers):\n",
    "            info.append({\n",
    "                'name': self.activation_names[i],\n",
    "                'params': self.activation_params_list[i]\n",
    "            })\n",
    "        return info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822b2d08",
   "metadata": {},
   "source": [
    "## 3. DEFINE  DATASET CLASS (e.g., PolymerToxicityDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a016738",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolymerToxicityDataset(Dataset):\n",
    "    def __init__(self, dataframe, fingerprint_col='fingerprints', property_cols=None, target_col='Hazard_Criteria_encoded'):\n",
    "        \"\"\"\n",
    "        Initialize the dataset.\n",
    "        \n",
    "        Args:\n",
    "            dataframe: Pandas DataFrame containing the data\n",
    "            fingerprint_col: Column name for fingerprint vectors\n",
    "            property_cols: List of property column names (one-hot encoded)\n",
    "            target_col: Column name for the target variable\n",
    "        \"\"\"\n",
    "        self.dataframe = dataframe\n",
    "        self.fingerprint_col = fingerprint_col\n",
    "        self.property_cols = property_cols if property_cols else []\n",
    "        self.target_col = target_col\n",
    "        \n",
    "        # Convert string representation of lists to actual lists for fingerprints\n",
    "        self.fingerprints = self.dataframe[fingerprint_col].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x).tolist()\n",
    "        \n",
    "        # Get property features if available\n",
    "        if self.property_cols:\n",
    "            self.properties = self.dataframe[self.property_cols].values\n",
    "        \n",
    "        # Get targets\n",
    "        self.targets = self.dataframe[target_col].values\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get fingerprint features\n",
    "        fingerprint = torch.tensor(self.fingerprints[idx], dtype=torch.float32)\n",
    "        \n",
    "        # Combine with property features if available\n",
    "        if self.property_cols:\n",
    "            properties = torch.tensor(self.properties[idx], dtype=torch.float32)\n",
    "            features = torch.cat([fingerprint, properties], dim=0)\n",
    "        else:\n",
    "            features = fingerprint\n",
    "        \n",
    "        # Get target\n",
    "        target = torch.tensor(self.targets[idx], dtype=torch.long)\n",
    "        \n",
    "        return features, target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7aa566",
   "metadata": {},
   "source": [
    "## 3. DEFINE ALL UTILITY FUNCTIONS\n",
    "\n",
    "(Place ALL your function definitions here, in this general order of dependency)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d83261",
   "metadata": {},
   "source": [
    "## 3.1  Utility function to compute class weights, traiing history and confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612ddf96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute class weights for imbalanced data\n",
    "def compute_class_weights(targets):\n",
    "    \"\"\"\n",
    "    Compute class weights inversely proportional to class frequencies.\n",
    "    \n",
    "    Args:\n",
    "        targets: Target labels\n",
    "        \n",
    "    Returns:\n",
    "        Tensor of class weights\n",
    "    \"\"\"\n",
    "    class_counts = np.bincount(targets)\n",
    "    total_samples = len(targets)\n",
    "    \n",
    "    # Compute weights inversely proportional to class frequencies\n",
    "    weights = total_samples / (len(class_counts) * class_counts)\n",
    "    \n",
    "    # Normalize weights\n",
    "    weights = weights / weights.sum()\n",
    "    \n",
    "    return torch.tensor(weights, dtype=torch.float32).to(device)\n",
    "\n",
    "# Plot training history\n",
    "def plot_training_history(history, save_path=None):\n",
    "    \"\"\"\n",
    "    Plot training and validation loss and accuracy.\n",
    "    \n",
    "    Args:\n",
    "        history: Training history dictionary\n",
    "        save_path: Path to save the plot\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Plot loss\n",
    "    ax1.plot(history['train_loss'], label='Train Loss')\n",
    "    ax1.plot(history['val_loss'], label='Validation Loss')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.set_title('Training and Validation Loss')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # Plot accuracy\n",
    "    ax2.plot(history['train_acc'], label='Train Accuracy')\n",
    "    ax2.plot(history['val_acc'], label='Validation Accuracy')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.set_title('Training and Validation Accuracy')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Plot confusion matrix\n",
    "def plot_confusion_matrix(cm, class_names, save_path=None):\n",
    "    \"\"\"\n",
    "    Plot confusion matrix.\n",
    "    \n",
    "    Args:\n",
    "        cm: Confusion matrix\n",
    "        class_names: List of class names\n",
    "        save_path: Path to save the plot\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix')\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0254a829",
   "metadata": {},
   "source": [
    "### Base model with best parameters including activation function support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a7d0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Place this in your \"4. DEFINE ALL UTILITY FUNCTIONS\" section\n",
    "# if you did not have it already.\n",
    "\n",
    "def create_enhanced_model_with_best_params(best_params, input_dim):\n",
    "    \"\"\"\n",
    "    Create enhanced model with best parameters including activation function support.\n",
    "    This function correctly extracts per-layer activation names and parameters\n",
    "    from the best_params dictionary generated by Optuna.\n",
    "    \"\"\"\n",
    "    n_layers = best_params['n_layers']\n",
    "    hidden_dims = [best_params[f'hidden_dim_{i}'] for i in range(n_layers)]\n",
    "    dropout_rates = [best_params[f'dropout_{i}'] for i in range(n_layers)]\n",
    "\n",
    "    # --- Section for extracting per-layer activations ---\n",
    "    activation_names = []\n",
    "    activation_params_list = []\n",
    "\n",
    "    for i in range(n_layers):\n",
    "        current_activation_name = best_params[f'activation_{i}']\n",
    "        activation_names.append(current_activation_name)\n",
    "\n",
    "        current_activation_params = {}\n",
    "        # Check if the specific activation parameter exists for this layer\n",
    "        if current_activation_name == 'PReLU':\n",
    "            if f'prelu_init_{i}' in best_params:\n",
    "                current_activation_params['init'] = best_params[f'prelu_init_{i}']\n",
    "        elif current_activation_name == 'LeakyReLU':\n",
    "            if f'leaky_relu_slope_{i}' in best_params:\n",
    "                current_activation_params['negative_slope'] = best_params[f'leaky_relu_slope_{i}']\n",
    "        elif current_activation_name == 'ELU':\n",
    "            if f'elu_alpha_{i}' in best_params:\n",
    "                current_activation_params['alpha'] = best_params[f'elu_alpha_{i}']\n",
    "        # Add checks for other activation types if they have parameters\n",
    "        \n",
    "        activation_params_list.append(current_activation_params)\n",
    "    # --- End of section for extracting per-layer activations ---\n",
    "\n",
    "    # Instantiate the ToxicityDNN model\n",
    "    # Ensure ToxicityDNN class is defined before this function\n",
    "    model = ToxicityDNN(input_dim, hidden_dims, dropout_rates, activation_names, activation_params_list)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8413f1c",
   "metadata": {},
   "source": [
    "### Data Distribution Visualization Functions\n",
    "\n",
    "Let's add functions to visualize the data distribution for both Tox21 and monomer datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa07a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data_distribution(dataset, dataset_name, class_names=None, save_path=None):\n",
    "    \"\"\"\n",
    "    Plot data distribution for a dataset.\n",
    "    \"\"\"\n",
    "    if class_names is None:\n",
    "        class_names = ['Not_Fulfilled', 'Fulfilled', 'Under_Investigation']\n",
    "    \n",
    "    # Extract targets from dataset\n",
    "    targets = []\n",
    "    for _, target in dataset:\n",
    "        targets.append(target.item())\n",
    "    \n",
    "    # Count classes\n",
    "    unique, counts = np.unique(targets, return_counts=True)\n",
    "    \n",
    "    # Create bar plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    colors = ['skyblue', 'lightcoral', 'lightgreen']\n",
    "    bars = plt.bar([class_names[i] for i in unique], counts, color=[colors[i] for i in unique])\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, count in zip(bars, counts):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(counts)*0.01,\n",
    "                str(count), ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    plt.xlabel('Hazard Criteria')\n",
    "    plt.ylabel('Number of Samples')\n",
    "    plt.title(f'{dataset_name} - Data Distribution')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add percentage labels\n",
    "    total = sum(counts)\n",
    "    for i, (bar, count) in enumerate(zip(bars, counts)):\n",
    "        percentage = (count / total) * 100\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height()/2,\n",
    "                f'{percentage:.1f}%', ha='center', va='center', fontweight='bold', color='white')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\n{dataset_name} Distribution Summary:\")\n",
    "    print(\"-\" * 40)\n",
    "    for i, count in zip(unique, counts):\n",
    "        percentage = (count / total) * 100\n",
    "        print(f\"{class_names[i]:20s}: {count:5d} samples ({percentage:5.1f}%)\")\n",
    "    print(f\"{'Total':20s}: {total:5d} samples\")\n",
    "\n",
    "def plot_tox21_splits_distribution(train_dataset, val_dataset, test_dataset, save_dir=None):\n",
    "    \"\"\"\n",
    "    Plot data distribution for Tox21 train, validation, and test splits.\n",
    "    \"\"\"\n",
    "    class_names = ['Not_Fulfilled', 'Fulfilled', 'Under_Investigation']\n",
    "    \n",
    "    # Create subplots\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    datasets = [train_dataset, val_dataset, test_dataset]\n",
    "    dataset_names = ['Training Set', 'Validation Set', 'Test Set']\n",
    "    colors = ['skyblue', 'lightcoral', 'lightgreen']\n",
    "    \n",
    "    for idx, (dataset, name, ax) in enumerate(zip(datasets, dataset_names, axes)):\n",
    "        # Extract targets\n",
    "        targets = []\n",
    "        for _, target in dataset:\n",
    "            targets.append(target.item())\n",
    "        \n",
    "        # Count classes\n",
    "        unique, counts = np.unique(targets, return_counts=True)\n",
    "        \n",
    "        # Create bar plot\n",
    "        bars = ax.bar([class_names[i] for i in unique], counts, color=[colors[i] for i in unique])\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, count in zip(bars, counts):\n",
    "            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(counts)*0.01,\n",
    "                   str(count), ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "        # Add percentage labels\n",
    "        total = sum(counts)\n",
    "        for i, (bar, count) in enumerate(zip(bars, counts)):\n",
    "            percentage = (count / total) * 100\n",
    "            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height()/2,\n",
    "                   f'{percentage:.1f}%', ha='center', va='center', fontweight='bold', color='white')\n",
    "        \n",
    "        ax.set_xlabel('Hazard Criteria')\n",
    "        ax.set_ylabel('Number of Samples')\n",
    "        ax.set_title(f'Tox21 {name}')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_dir:\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        plt.savefig(os.path.join(save_dir, 'tox21_splits_distribution.png'), dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def plot_monomer_distribution(monomer_dataset, save_path=None):\n",
    "    \"\"\"\n",
    "    Plot data distribution for monomer dataset.\n",
    "    \"\"\"\n",
    "    plot_data_distribution(monomer_dataset, \"Monomer Dataset\", save_path=save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573071b9",
   "metadata": {},
   "source": [
    "### 3.4 Evaluation Function (F1, recall, precision, confusion matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092f2b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced evaluation function\n",
    "def evaluate_model(model, test_loader, class_names=None):\n",
    "    \"\"\"\n",
    "    Evaluate the model on test data with comprehensive metrics.\n",
    "    \n",
    "    Args:\n",
    "        model: PyTorch model\n",
    "        test_loader: DataLoader for test data\n",
    "        class_names: List of class names for classification report\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary of evaluation metrics\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    all_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            \n",
    "            # Get predictions\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            # Collect results\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_targets.extend(targets.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_targets = np.array(all_targets)\n",
    "    all_probs = np.array(all_probs)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(all_targets, all_preds)\n",
    "    f1 = f1_score(all_targets, all_preds, average='weighted')\n",
    "    precision = precision_score(all_targets, all_preds, average='weighted')\n",
    "    recall = recall_score(all_targets, all_preds, average='weighted')\n",
    "    \n",
    "    # Calculate per-class metrics\n",
    "    f1_per_class = f1_score(all_targets, all_preds, average=None)\n",
    "    precision_per_class = precision_score(all_targets, all_preds, average=None)\n",
    "    recall_per_class = recall_score(all_targets, all_preds, average=None)\n",
    "    \n",
    "    # Calculate ROC-AUC (one vs rest)\n",
    "    try:\n",
    "        roc_auc = roc_auc_score(all_targets, all_probs, multi_class='ovr')\n",
    "        # Per-class ROC-AUC\n",
    "        roc_auc_per_class = roc_auc_score(all_targets, all_probs, multi_class='ovr', average=None)\n",
    "    except ValueError:\n",
    "        # Handle case where some classes might be missing\n",
    "        roc_auc = np.nan\n",
    "        roc_auc_per_class = [np.nan] * 3\n",
    "    \n",
    "    # Create confusion matrix\n",
    "    cm = confusion_matrix(all_targets, all_preds)\n",
    "    \n",
    "    # Classification report\n",
    "    if class_names is None:\n",
    "        class_names = ['Not_Fulfilled', 'Fulfilled', 'Under_Investigation']\n",
    "    \n",
    "    report = classification_report(all_targets, all_preds, target_names=class_names, output_dict=True)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'roc_auc': roc_auc,\n",
    "        'f1_per_class': f1_per_class,\n",
    "        'precision_per_class': precision_per_class,\n",
    "        'recall_per_class': recall_per_class,\n",
    "        'roc_auc_per_class': roc_auc_per_class,\n",
    "        'confusion_matrix': cm,\n",
    "        'classification_report': report,\n",
    "        'predictions': all_preds,\n",
    "        'targets': all_targets,\n",
    "        'probabilities': all_probs\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde717ca",
   "metadata": {},
   "source": [
    "### 3.5  ROC AUC Plotting Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432d5efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_roc_curves(y_true, y_prob, class_names, save_path=None, title='Receiver Operating Characteristic (ROC) Curve Frozenlayer Finetuned Model'):\n",
    "    \"\"\"\n",
    "    Plots ROC curves (One-vs-Rest and micro-average) for a multi-class problem.\n",
    "    \"\"\"\n",
    "    # Binarize the labels for one-vs-rest ROC calculation\n",
    "    y_true_bin = label_binarize(y_true, classes=range(len(class_names)))\n",
    "    n_classes = y_true_bin.shape[1]\n",
    "    \n",
    "    # Compute ROC curve and ROC area for each class\n",
    "    fpr, tpr, roc_auc = {}, {}, {}\n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_true_bin[:, i], y_prob[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "    # Compute micro-average ROC curve and ROC area\n",
    "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_true_bin.ravel(), y_prob.ravel())\n",
    "    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "    # Plot all ROC curves\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    colors = ['aqua', 'darkorange', 'cornflowerblue']\n",
    "    \n",
    "    for i, color in enumerate(colors):\n",
    "        plt.plot(fpr[i], tpr[i], color=color, lw=2,\n",
    "                 label=f'ROC curve of {class_names[i]} vs rest (area = {roc_auc[i]:0.2f})')\n",
    "\n",
    "    plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "             label=f'micro-average ROC curve (area = {roc_auc[\"micro\"]:.2f})',\n",
    "             color='deeppink', linestyle=':', linewidth=4)\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(title)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300)\n",
    "        \n",
    "    plt.show()\n",
    "\n",
    "    # Print the summary scores\n",
    "    print(\"\\nModel ROC-AUC scores:\")\n",
    "    for i, name in enumerate(class_names):\n",
    "        print(f\"{name} vs rest: {roc_auc[i]:.4f}\")\n",
    "    print(f\"Micro-average: {roc_auc['micro']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4abba87",
   "metadata": {},
   "source": [
    "### 3.6 simplified training function for fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04563b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Paste the utility functions here)\n",
    "\n",
    "# Add this simplified training function for fine-tuning\n",
    "def fine_tune_model(model, train_loader, criterion, optimizer, num_epochs=10):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a83b58",
   "metadata": {},
   "source": [
    "## 4. Load and Prepare Polymer Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246fbe68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the polymer data\n",
    "polymer_df = pd.read_csv(POLYMER_DATA_PATH)\n",
    "print(f\"Polymer dataset loaded with {len(polymer_df)} samples.\")\n",
    "\n",
    "# Extract features and target columns\n",
    "property_cols = [col for col in polymer_df.columns if col.startswith('Property_')]\n",
    "polymer_dataset = PolymerToxicityDataset(polymer_df, 'fingerprints', property_cols, 'Hazard_Criteria_encoded')\n",
    "\n",
    "# Get the input dimension from the first sample\n",
    "sample_features, _ = polymer_dataset[0]\n",
    "input_dim = sample_features.shape[0]\n",
    "\n",
    "# Prepare X and y numpy arrays for splitting\n",
    "X_polymer = []\n",
    "y_polymer = []\n",
    "for features, target in polymer_dataset:\n",
    "    X_polymer.append(features.numpy())\n",
    "    y_polymer.append(target.item())\n",
    "\n",
    "X_polymer = np.array(X_polymer)\n",
    "y_polymer = np.array(y_polymer)\n",
    "\n",
    "print(f\"Polymer data prepared. X shape: {X_polymer.shape}, y shape: {y_polymer.shape}\")\n",
    "print(f\"Polymer dataset loaded with {len(polymer_dataset)} samples.\")\n",
    "print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a080598",
   "metadata": {},
   "source": [
    "### 5.  Data Splitting for Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc08b111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- NEW UNIVERSAL DATA SPLITTING CELL ---\n",
    "\n",
    "# Define the sizes for each set based on the new strategy\n",
    "n_total = len(X_polymer)\n",
    "n_test = 0.2  # 20 samples for the final noble test set\n",
    "n_finetune = 0.2 # 20 samples for the initial fine-tuning\n",
    "\n",
    "# Step 1: Split off the \"Noble Test Set\"\n",
    "# This creates a main training pool (76 samples) and the final test set (20 samples).\n",
    "X_train_pool, X_test, y_train_pool, y_test = train_test_split(\n",
    "    X_polymer,\n",
    "    y_polymer,\n",
    "    test_size=n_test,\n",
    "    random_state=RANDOM_SEED,\n",
    "    stratify=y_polymer\n",
    ")\n",
    "\n",
    "# Step 2: Split the training pool into a \"Fine-Tuning Set\" and a \"Cross-Validation Set\"\n",
    "# This takes the 76 samples and splits them into 20 for fine-tuning and 56 for the CV.\n",
    "X_cv, X_finetune, y_cv, y_finetune = train_test_split(\n",
    "    X_train_pool,\n",
    "    y_train_pool,\n",
    "    test_size=n_finetune,\n",
    "    random_state=RANDOM_SEED,\n",
    "    stratify=y_train_pool\n",
    ")\n",
    "\n",
    "# --- Verification ---\n",
    "print(\"Data has been split into three distinct sets:\")\n",
    "print(f\" - Fine-Tuning Set: {len(X_finetune)} samples\")\n",
    "print(f\" - Cross-Validation Set: {len(X_cv)} samples\")\n",
    "print(f\" - Noble Test Set (Final Evaluation): {len(X_test)} samples\")\n",
    "print(f\"Total: {len(X_finetune) + len(X_cv) + len(X_test)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f22acb",
   "metadata": {},
   "source": [
    "### 6. Load the Model and Freeze Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa014893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best hyperparameters from the JSON file\n",
    "with open(PARAMS_PATH, 'r') as f:\n",
    "    best_params = json.load(f)\n",
    "\n",
    "# Load the pre-trained base model\n",
    "base_model = create_enhanced_model_with_best_params(best_params, input_dim)\n",
    "base_model.load_state_dict(torch.load(BASE_MODEL_PATH))\n",
    "\n",
    "print(\"Freezing the first two layers of the model...\")\n",
    "layers_to_freeze = ['linear_layers.0']   # Corresponds to the first nn.Sequential block    2nd layer # 'linear_layers.1'\n",
    "\n",
    "for name, param in base_model.named_parameters():\n",
    "    # Check if the parameter's name starts with any of the layer names to freeze\n",
    "    if any(name.startswith(layer) for layer in layers_to_freeze):\n",
    "        param.requires_grad = False\n",
    "        print(f\"  -> Froze layer: {name}\")\n",
    "    else:\n",
    "        param.requires_grad = True\n",
    "        print(f\"  -> Layer remains trainable: {name}\")\n",
    "        \n",
    "# We will use this 'base_model' with its frozen layers as a template in our CV loop\n",
    "print(\"Model preparation complete. Layers are frozen.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e1aba6",
   "metadata": {},
   "source": [
    "### 7. 5-Fold Cross-Validation for Frozen Layer Finetuning  Evaluation\n",
    "\n",
    "Use the X_eval and y_eval data (the remaining 77 data points).\n",
    "\n",
    " In each fold, we will further split the data into 80% for training and 20% for testing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c0031b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5-Fold Cross-Validation on the CV Set (56 samples) ---\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_SEED)\n",
    "summary_metrics = []\n",
    "all_fold_preds = np.array([])\n",
    "all_fold_targets = np.array([])\n",
    "all_fold_probs = None\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Starting 5-Fold Cross-Validation on the CV Set\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# The CV loop now runs on X_cv and y_cv\n",
    "for fold, (train_idx, test_idx) in enumerate(skf.split(X_cv, y_cv)):\n",
    "    print(f\"\\nFold {fold + 1}/5\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    # Use the CV data for this fold\n",
    "    X_train_fold, X_test_fold = X_cv[train_idx], X_cv[test_idx]\n",
    "    y_train_fold, y_test_fold = y_cv[train_idx], y_cv[test_idx]\n",
    "\n",
    "    print(f\"Train samples for this fold: {len(X_train_fold)}\")\n",
    "    print(f\"Test samples for this fold: {len(X_test_fold)}\")\n",
    "    \n",
    "    # Use a fresh, deep copy of the prepared base_model for each fold\n",
    "    model_for_fold = copy.deepcopy(base_model).to(device)\n",
    "    \n",
    "    # Create DataLoaders\n",
    "    train_dataset_fold = torch.utils.data.TensorDataset(torch.tensor(X_train_fold, dtype=torch.float32), torch.tensor(y_train_fold, dtype=torch.long))\n",
    "    train_loader_fold = DataLoader(train_dataset_fold, batch_size=best_params['batch_size'], shuffle=True)\n",
    "    test_dataset_fold = torch.utils.data.TensorDataset(torch.tensor(X_test_fold, dtype=torch.float32), torch.tensor(y_test_fold, dtype=torch.long))\n",
    "    test_loader_fold = DataLoader(test_dataset_fold, batch_size=best_params['batch_size'], shuffle=False)\n",
    "\n",
    "    # # Train the model copy on this fold's training data\n",
    "    # optimizer_fold = torch.optim.Adam(model_for_fold.parameters(), lr=1e-5)\n",
    "    # Create an optimizer that only updates the TRAINABLE (unfrozen) layers\n",
    "    optimizer_fold = torch.optim.Adam(\n",
    "        filter(lambda p: p.requires_grad, model_for_fold.parameters()), \n",
    "        lr=1e-4  # A slightly higher LR can be used when fine-tuning fewer layers\n",
    "    )\n",
    "\n",
    "    criterion_fold = nn.CrossEntropyLoss()\n",
    "    trained_fold_model = fine_tune_model(model_for_fold, train_loader_fold, criterion_fold, optimizer_fold, num_epochs=50)\n",
    "\n",
    "    # Evaluate on the test set of the fold\n",
    "    fold_metrics = evaluate_model(trained_fold_model, test_loader_fold, class_names)\n",
    "\n",
    "    # Print results and aggregate for summary\n",
    "    print(f\"Accuracy:  {fold_metrics['accuracy']:.4f}\")\n",
    "    print(f\"F1 Score:  {fold_metrics['f1']:.4f}\")\n",
    "    print(f\"ROC-AUC:   {fold_metrics['roc_auc']:.4f}\")\n",
    "    summary_metrics.append(fold_metrics)\n",
    "    all_fold_preds = np.concatenate([all_fold_preds, fold_metrics['predictions']])\n",
    "    all_fold_targets = np.concatenate([all_fold_targets, fold_metrics['targets']])\n",
    "    if all_fold_probs is None:\n",
    "        all_fold_probs = fold_metrics['probabilities']\n",
    "    else:\n",
    "        all_fold_probs = np.vstack([all_fold_probs, fold_metrics['probabilities']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707783bf",
   "metadata": {},
   "source": [
    "### 8. Summary Statistics and Final Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea9f90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Summary statistics from the 5-Fold CV on the development set ---\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"MODEL ROBUSTNESS: SUMMARY STATISTICS ACROSS CV FOLDS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Extract each metric from the list of dictionaries\n",
    "accuracies = [m['accuracy'] for m in summary_metrics]\n",
    "precisions = [m['precision'] for m in summary_metrics]\n",
    "recalls = [m['recall'] for m in summary_metrics]\n",
    "f1s = [m['f1'] for m in summary_metrics]\n",
    "roc_aucs = [m['roc_auc'] for m in summary_metrics]\n",
    "\n",
    "# Calculate and print the stats in the desired format\n",
    "print(f\"ACCURACY  : {np.mean(accuracies):.4f} ¬± {np.std(accuracies):.4f} (min: {np.min(accuracies):.4f}, max: {np.max(accuracies):.4f})\")\n",
    "print(f\"PRECISION : {np.mean(precisions):.4f} ¬± {np.std(precisions):.4f} (min: {np.min(precisions):.4f}, max: {np.max(precisions):.4f})\")\n",
    "print(f\"RECALL    : {np.mean(recalls):.4f} ¬± {np.std(recalls):.4f} (min: {np.min(recalls):.4f}, max: {np.max(recalls):.4f})\")\n",
    "print(f\"F1        : {np.mean(f1s):.4f} ¬± {np.std(f1s):.4f} (min: {np.min(f1s):.4f}, max: {np.max(f1s):.4f})\")\n",
    "print(f\"ROC_AUC   : {np.mean(roc_aucs):.4f} ¬± {np.std(roc_aucs):.4f} (min: {np.min(roc_aucs):.4f}, max: {np.max(roc_aucs):.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7339ee9",
   "metadata": {},
   "source": [
    "##### 9. Train Final Model and Evaluate on the Noble Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7be2e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Train final model on the entire CV set (56 samples) ---\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Training final model on the entire CV set (56 samples)...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create a fresh copy of the initial prepared model\n",
    "final_model = copy.deepcopy(base_model).to(device)\n",
    "\n",
    "# Create a DataLoader for the full CV dataset\n",
    "cv_dataset = torch.utils.data.TensorDataset(torch.tensor(X_cv, dtype=torch.float32), torch.tensor(y_cv, dtype=torch.long))\n",
    "cv_loader = DataLoader(cv_dataset, batch_size=best_params['batch_size'], shuffle=True)\n",
    "\n",
    "# Create the optimizer for the trainable layers\n",
    "final_optimizer = torch.optim.Adam(\n",
    "    filter(lambda p: p.requires_grad, final_model.parameters()), \n",
    "    lr=1e-4\n",
    ")\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train the model on the full CV set\n",
    "trained_final_model = fine_tune_model(final_model, cv_loader, criterion, final_optimizer, num_epochs=50)\n",
    "print(\"Final model training complete.\")\n",
    "\n",
    "# --- Final Evaluation Report on the NOBLE TEST SET ---\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"FINAL PERFORMANCE ON UNSEEN NOBLE TEST SET (20 SAMPLES)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create a DataLoader for the noble test set and evaluate\n",
    "test_dataset_noble = torch.utils.data.TensorDataset(torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.long))\n",
    "test_loader_noble = DataLoader(test_dataset_noble, batch_size=best_params['batch_size'], shuffle=False)\n",
    "final_metrics = evaluate_model(trained_final_model, test_loader_noble, class_names)\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\nDetailed Classification Report for Final Model:\")\n",
    "print(\"-\" * 60)\n",
    "print(classification_report(final_metrics['targets'], final_metrics['predictions'], target_names=class_names))\n",
    "\n",
    "# Confusion Matrix and other plots...\n",
    "# (Your existing code for plotting the confusion matrix, per-class accuracy, and ROC curve is correct and goes here)\n",
    "\n",
    "# Confusion Matrix\n",
    "print(\"\\nConfusion Matrix for Final Model:\")\n",
    "cm = confusion_matrix(final_metrics['targets'], final_metrics['predictions'])\n",
    "plot_confusion_matrix(cm, class_names, save_path=os.path.join(RESULTS_DIR, \"frozenlayerfinetuning_confusion_matrix.png\"))\n",
    "\n",
    "# Per-class Accuracy\n",
    "print(\"\\nPer-class Accuracy for Final Model:\")\n",
    "print(\"-\" * 40)\n",
    "for i, name in enumerate(class_names):\n",
    "    true_samples_for_class = cm[i].sum()\n",
    "    if true_samples_for_class > 0:\n",
    "        class_accuracy = cm[i, i] / true_samples_for_class\n",
    "        print(f\"{name:25s}: {class_accuracy:.4f} ({cm[i, i]}/{true_samples_for_class})\")\n",
    "    else:\n",
    "        print(f\"{name:25s}: No test samples in this class\")\n",
    "overall_accuracy = np.trace(cm) / np.sum(cm)\n",
    "print(f\"\\nOverall Accuracy: {overall_accuracy:.4f}\")\n",
    "\n",
    "\n",
    "# ROC AUC Curve\n",
    "print(\"\\nROC AUC Curve (One vs Rest) for Final Model:\")\n",
    "plot_roc_curves(final_metrics['targets'], final_metrics['probabilities'], class_names, save_path=os.path.join(RESULTS_DIR, \"final_roc_curve.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cea8485",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe53dc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0a6e1044",
   "metadata": {},
   "source": [
    "### Final Training and Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3273e360",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PERFORMING FINAL TRAINING ON ALL 96 DATA POINTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. Create the final model instance and freeze its layers\n",
    "final_model = create_enhanced_model_with_best_params(best_params, input_dim)\n",
    "final_model.load_state_dict(torch.load(BASE_MODEL_PATH))\n",
    "\n",
    "for name, param in final_model.named_parameters():\n",
    "    if any(name.startswith(layer) for layer in layers_to_freeze):\n",
    "        param.requires_grad = False\n",
    "    else:\n",
    "        param.requires_grad = True\n",
    "\n",
    "final_model.to(device)\n",
    "\n",
    "# 2. Create a DataLoader with all 96 polymer data points\n",
    "full_dataset = PolymerToxicityDataset(polymer_df, 'fingerprints', property_cols, 'Hazard_Criteria_encoded')\n",
    "full_train_loader = DataLoader(full_dataset, batch_size=best_params['batch_size'], shuffle=True)\n",
    "\n",
    "# 3. Create the optimizer for the trainable layers\n",
    "final_optimizer = torch.optim.Adam(\n",
    "    filter(lambda p: p.requires_grad, final_model.parameters()),\n",
    "    lr=1e-4\n",
    ")\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 4. Fine-tune the model on the full dataset\n",
    "print(\"Fine-tuning the unfrozen layers on all data...\")\n",
    "trained_final_model = fine_tune_model(final_model, full_train_loader, criterion, final_optimizer, num_epochs=50)\n",
    "print(\"Final frozen and fine-tuning complete.\")\n",
    "\n",
    "# 5. Save the final model with a descriptive name\n",
    "save_path = os.path.join(RESULTS_DIR, \"frozen_finetuned_96shot_model.pt\")\n",
    "torch.save(trained_final_model.state_dict(), save_path)\n",
    "print(f\"Final frozen-layer model successfully saved to: {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad67c8a1",
   "metadata": {},
   "source": [
    "### 8. Summary Statistics and Final Reports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8d1111",
   "metadata": {},
   "source": [
    "## 8.1  The current method aggregates the results from all 5 folds to show one final confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7db9e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Summary statistics across all folds ---\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SUMMARY STATISTICS ACROSS ALL FOLDS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Extract each metric from the list of dictionaries\n",
    "accuracies = [m['accuracy'] for m in summary_metrics]\n",
    "precisions = [m['precision'] for m in summary_metrics]\n",
    "recalls = [m['recall'] for m in summary_metrics]\n",
    "f1s = [m['f1'] for m in summary_metrics]\n",
    "roc_aucs = [m['roc_auc'] for m in summary_metrics]\n",
    "\n",
    "# Calculate and print the stats in the desired format\n",
    "print(f\"ACCURACY  : {np.mean(accuracies):.4f} ¬± {np.std(accuracies):.4f} (min: {np.min(accuracies):.4f}, max: {np.max(accuracies):.4f})\")\n",
    "print(f\"PRECISION : {np.mean(precisions):.4f} ¬± {np.std(precisions):.4f} (min: {np.min(precisions):.4f}, max: {np.max(precisions):.4f})\")\n",
    "print(f\"RECALL    : {np.mean(recalls):.4f} ¬± {np.std(recalls):.4f} (min: {np.min(recalls):.4f}, max: {np.max(recalls):.4f})\")\n",
    "print(f\"F1        : {np.mean(f1s):.4f} ¬± {np.std(f1s):.4f} (min: {np.min(f1s):.4f}, max: {np.max(f1s):.4f})\")\n",
    "print(f\"ROC_AUC   : {np.mean(roc_aucs):.4f} ¬± {np.std(roc_aucs):.4f} (min: {np.min(roc_aucs):.4f}, max: {np.max(roc_aucs):.4f})\")\n",
    "\n",
    "\n",
    "# --- Final Evaluation Report on all test folds combined ---\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"FINAL FROZEN-LAYER EVALUATION ON ALL TEST FOLDS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\nDetailed Classification Report forFrozen-Layer Model:\")\n",
    "print(\"-\" * 60)\n",
    "print(classification_report(all_fold_targets, all_fold_preds, target_names=class_names))\n",
    "\n",
    "# The rest of Cell 8 (plotting functions) would follow...\n",
    "# Confusion Matrix\n",
    "print(\"\\nConfusion Matrix for Frozen-Layer Model:\")\n",
    "cm = confusion_matrix(all_fold_targets, all_fold_preds)\n",
    "plot_confusion_matrix(cm, class_names, save_path=os.path.join(RESULTS_DIR, \"Frozen-Layer_confusion_matrix.png\"))\n",
    "\n",
    "# Per-class Accuracy\n",
    "print(\"\\nPer-class Accuracy for Frozen-Layer Model:\")\n",
    "print(\"-\" * 40)\n",
    "for i, name in enumerate(class_names):\n",
    "    # Calculate the number of true samples for the current class\n",
    "    true_samples_for_class = cm[i].sum()\n",
    "\n",
    "    if true_samples_for_class > 0:\n",
    "        # Calculate accuracy for the class\n",
    "        class_accuracy = cm[i, i] / true_samples_for_class\n",
    "        print(f\"{name:25s}: {class_accuracy:.4f} ({cm[i, i]}/{true_samples_for_class})\")\n",
    "    else:\n",
    "        print(f\"{name:25s}: No test samples in this class\")\n",
    "\n",
    "# Overall accuracy from the confusion matrix\n",
    "overall_accuracy = np.trace(cm) / np.sum(cm)\n",
    "print(f\"\\nOverall Accuracy: {overall_accuracy:.4f}\")\n",
    "\n",
    "# ROC AUC Curve\n",
    "print(\"\\nROC AUC Curve (One vs Rest) for Frozen-Layer Model:\")\n",
    "plot_roc_curves(all_fold_targets, all_fold_probs, class_names, save_path=os.path.join(RESULTS_DIR, \"Frozen-Layer_roc_curve.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc030d4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565b9eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_split.py\n",
    "# Usage: python plot_split.py\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def make_plot(save_path=\"training_testing_per_fold_UPDATED.png\", use_emojis=True):\n",
    "    # Labels\n",
    "    few_shot = \"Few-Shot Model\" if use_emojis else \"Few-Shot\"\n",
    "    frozen   = \"Frozen-Layer Fine Tuning\" if use_emojis else \"Frozen-Layer\"\n",
    "    models = [few_shot, frozen]\n",
    "\n",
    "    # Data (per fold) + metadata\n",
    "    train_per_fold = [60, 60]\n",
    "    test_per_fold  = [20, 20]\n",
    "    fine_tune      = [16, 16]\n",
    "    totals         = [96, 96]\n",
    "\n",
    "    x = np.arange(len(models))\n",
    "    width = 0.36\n",
    "\n",
    "    # Figure\n",
    "    fig, ax = plt.subplots(figsize=(9, 4.2))\n",
    "\n",
    "    # Bars: train vs test (grouped)\n",
    "    bars_train = ax.bar(x - width/2, train_per_fold, width, label=\"Train / fold\")\n",
    "    bars_test  = ax.bar(x + width/2, test_per_fold,  width, label=\"Test / fold\")\n",
    "\n",
    "    # Value labels on bars\n",
    "    def autolabel(rects):\n",
    "        for r in rects:\n",
    "            h = r.get_height()\n",
    "            ax.text(r.get_x() + r.get_width()/2, h + 1.2, f\"{int(h)}\",\n",
    "                    ha=\"center\", va=\"bottom\", fontsize=10)\n",
    "    autolabel(bars_train)\n",
    "    autolabel(bars_test)\n",
    "\n",
    "    # Titles & axes\n",
    "    ax.set_title(\"Training vs Testing data split for Fewshot vs Frozen layer Fine tuning\", fontsize=18, pad=20)\n",
    "    ax.set_ylabel(\"Samples\")\n",
    "    ax.set_xticks(x, models)\n",
    "    ax.set_ylim(0, 75)  # ensures room for annotations\n",
    "    ax.legend(loc=\"upper right\", frameon=False)\n",
    "\n",
    "    # Non-overlapping annotations for fine-tuning & totals\n",
    "    for i, (ft, tot) in enumerate(zip(fine_tune, totals)):\n",
    "        ax.text(x[i] - 0.35, 71.5, f\"Fine-tuning: {ft}\",\n",
    "                ha=\"left\", va=\"center\", fontsize=10)\n",
    "        ax.text(x[i] - 0.35, 67.5, f\"Total: {tot}\",\n",
    "                ha=\"left\", va=\"center\", fontsize=10)\n",
    "\n",
    "    # Layout and save\n",
    "    plt.subplots_adjust(top=0.82, left=0.10, right=0.98, bottom=0.22)\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches=\"tight\")\n",
    "    print(f\"Saved figure to: {save_path}\")\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set use_emojis=False if your environment/font has issues with emoji glyphs\n",
    "    make_plot(save_path=\"training_testing data_Split.png\", use_emojis=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
