{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea10d9fa",
   "metadata": {},
   "source": [
    "# Fewshot Model with 20% Ploymer data Fine tuned on Only Molecule model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5830597f",
   "metadata": {},
   "source": [
    "##  GPU Avaiablity Chcek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4336fb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import torch\n",
    "\n",
    "def get_filtered_free_gpus(allowed_gpus={0, 2, 3}):\n",
    "    \"\"\"Return (gpu_id, free_mem_MB) from allowed_gpus, sorted by free memory.\"\"\"\n",
    "    try:\n",
    "        result = subprocess.check_output(\n",
    "            ['nvidia-smi', '--query-gpu=index,memory.free', '--format=csv,noheader,nounits'],\n",
    "            encoding='utf-8'\n",
    "        )\n",
    "        gpu_info = []\n",
    "        for line in result.strip().split('\\n'):\n",
    "            gpu_id, mem_free = line.strip().split(',')\n",
    "            gpu_id = int(gpu_id)\n",
    "            mem_free = int(mem_free)\n",
    "            if gpu_id in allowed_gpus:\n",
    "                gpu_info.append((gpu_id, mem_free))\n",
    "        return sorted(gpu_info, key=lambda x: x[1], reverse=True)\n",
    "    except Exception as e:\n",
    "        print(\"‚ö†Ô∏è Could not query GPU info:\", e)\n",
    "        return []\n",
    "\n",
    "# Step 1: Get preferred GPU (among 0, 2, 3)\n",
    "available_gpus = get_filtered_free_gpus()\n",
    "\n",
    "if available_gpus:\n",
    "    selected_system_gpu = available_gpus[0][0]\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(selected_system_gpu)\n",
    "    print(f\"üß† Selected system GPU ID: {selected_system_gpu} with {available_gpus[0][1]} MB free memory.\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No preferred GPUs available. Using CPU.\")\n",
    "\n",
    "# Step 2: PyTorch setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    current_device = torch.cuda.current_device()\n",
    "    visible_device_name = torch.cuda.get_device_name(current_device)\n",
    "    visible_ids = os.environ[\"CUDA_VISIBLE_DEVICES\"].split(\",\")\n",
    "    real_system_gpu_id = visible_ids[current_device]\n",
    "\n",
    "    print(\"‚úÖ GPU is available!\")\n",
    "    print(f\"üñ•Ô∏è Visible PyTorch Device: cuda:{current_device}\")\n",
    "    print(f\"üß≠ Actual system GPU ID: {real_system_gpu_id}\")\n",
    "    print(f\"üìü GPU Name: {visible_device_name}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è GPU not available. Using CPU.\")\n",
    "\n",
    "print(f\"Model will run on: {device}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e551d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(\"My PID:\", os.getpid())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed707abf",
   "metadata": {},
   "source": [
    "## 1. # IMPORTS AND GLOBAL CONFIGURATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b008508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, precision_score, recall_score,\n",
    "    roc_auc_score, confusion_matrix, classification_report\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import ast\n",
    "from tqdm.notebook import tqdm\n",
    "import warnings\n",
    "import copy\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "\n",
    "print(\"Libraries imported and random seed set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0b6ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CONFIGURATION AND PATHS FOR FEW-SHOT ---\n",
    "BASE_MODEL_PATH = \"/home/sunil/am2/poetry-demo/Polytox_Matser_Thesis/Few_shot_learning_with_validation_data_Monomers/Fewshot_DNN_Model_2/models/base_model.pt\"\n",
    "PARAMS_PATH = \"/home/sunil/am2/poetry-demo/Polytox_Matser_Thesis/Few_shot_learning_with_validation_data_Monomers/Fewshot_DNN_Model_2/optuna_results/best_params.json\"\n",
    "POLYMER_DATA_PATH = \"/home/sunil/am2/poetry-demo/Polytox_Matser_Thesis/Few_shot_learning_with_validation_data_Monomers/Fewshot_DNN_Model_2/polymer_DNN_encoded_final_data.csv\"\n",
    "\n",
    "# Directory to save new results\n",
    "RESULTS_DIR = \"./results_fewshot\"\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "# Define class names for plotting and reports\n",
    "class_names = ['Not_Fulfilled', 'Fulfilled', 'Under_Investigation']\n",
    "\n",
    "print(\"Configuration for Few-Shot evaluation is set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c72576",
   "metadata": {},
   "source": [
    "## 2. DEFINE YOUR DNN MODEL CLASS(ToxiciityDNN CLASS DEFINITION )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d231705f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ToxicityDNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, dropout_rates, activation_names, activation_params_list):\n",
    "        super(ToxicityDNN, self).__init__()\n",
    "        self.n_layers = len(hidden_dims)\n",
    "        self.dropout_rates = dropout_rates\n",
    "        self.activation_names = activation_names # Store for potential later inspection if needed\n",
    "        self.activation_params_list = activation_params_list # Store for potential later inspection if needed\n",
    "\n",
    "        self.activation_modules = nn.ModuleList() # Use ModuleList for activations\n",
    "\n",
    "        # Combine Linear and Dropout layers into a single ModuleList for simpler iteration\n",
    "        self.linear_layers = nn.ModuleList()\n",
    "        self.dropout_layers = nn.ModuleList()\n",
    "\n",
    "        in_dim = input_dim\n",
    "\n",
    "        for i in range(self.n_layers):\n",
    "            out_dim = hidden_dims[i]\n",
    "            self.linear_layers.append(nn.Linear(in_dim, out_dim)) # Append linear layer\n",
    "\n",
    "            # Create activation module for the current layer\n",
    "            activation_name = activation_names[i]\n",
    "            # Safely get params for the current layer, default to empty dict if index out of bounds\n",
    "            activation_params = activation_params_list[i] if i < len(activation_params_list) else {}\n",
    "\n",
    "            if activation_name == 'ReLU':\n",
    "                self.activation_modules.append(nn.ReLU())\n",
    "            elif activation_name == 'PReLU':\n",
    "                # num_parameters should match the input feature dimension if you want one param per feature\n",
    "                # or just 1 if shared across all features. For simplicity, we'll keep 1 as in your original\n",
    "                self.activation_modules.append(nn.PReLU(num_parameters=1, init=activation_params.get('init', 0.25)))\n",
    "            elif activation_name == 'SiLU':\n",
    "                self.activation_modules.append(nn.SiLU())\n",
    "            elif activation_name == 'LeakyReLU':\n",
    "                self.activation_modules.append(nn.LeakyReLU(negative_slope=activation_params.get('negative_slope', 0.01)))\n",
    "            elif activation_name == 'ELU':\n",
    "                self.activation_modules.append(nn.ELU(alpha=activation_params.get('alpha', 1.0)))\n",
    "            elif activation_name == 'GELU':\n",
    "                self.activation_modules.append(nn.GELU())\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown activation function: {activation_name}\")\n",
    "\n",
    "            self.dropout_layers.append(nn.Dropout(dropout_rates[i])) # Append dropout layer\n",
    "            in_dim = out_dim\n",
    "\n",
    "        # Output layer\n",
    "        self.output_layer = nn.Linear(in_dim, 3) # Assuming 3 output classes\n",
    "\n",
    "    # ONLY ONE FORWARD METHOD\n",
    "    def forward(self, x):\n",
    "        for i in range(self.n_layers):\n",
    "            x = self.linear_layers[i](x)      # Apply Linear layer\n",
    "            x = self.activation_modules[i](x) # Apply specific activation for this layer\n",
    "            x = self.dropout_layers[i](x)     # Apply Dropout layer\n",
    "\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "    # Remove or modify get_activation_info if you need it to reflect per-layer info\n",
    "    # For example, to get info for all layers:\n",
    "    def get_all_activation_info(self):\n",
    "        \"\"\"\n",
    "        Get information about activation functions used in all layers.\n",
    "        \"\"\"\n",
    "        info = []\n",
    "        for i in range(self.n_layers):\n",
    "            info.append({\n",
    "                'name': self.activation_names[i],\n",
    "                'params': self.activation_params_list[i]\n",
    "            })\n",
    "        return info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822b2d08",
   "metadata": {},
   "source": [
    "## 3. DEFINE  DATASET CLASS (e.g., PolymerToxicityDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a016738",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolymerToxicityDataset(Dataset):\n",
    "    def __init__(self, dataframe, fingerprint_col='fingerprints', property_cols=None, target_col='Hazard_Criteria_encoded'):\n",
    "        \"\"\"\n",
    "        Initialize the dataset.\n",
    "        \n",
    "        Args:\n",
    "            dataframe: Pandas DataFrame containing the data\n",
    "            fingerprint_col: Column name for fingerprint vectors\n",
    "            property_cols: List of property column names (one-hot encoded)\n",
    "            target_col: Column name for the target variable\n",
    "        \"\"\"\n",
    "        self.dataframe = dataframe\n",
    "        self.fingerprint_col = fingerprint_col\n",
    "        self.property_cols = property_cols if property_cols else []\n",
    "        self.target_col = target_col\n",
    "        \n",
    "        # Convert string representation of lists to actual lists for fingerprints\n",
    "        self.fingerprints = self.dataframe[fingerprint_col].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x).tolist()\n",
    "        \n",
    "        # Get property features if available\n",
    "        if self.property_cols:\n",
    "            self.properties = self.dataframe[self.property_cols].values\n",
    "        \n",
    "        # Get targets\n",
    "        self.targets = self.dataframe[target_col].values\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get fingerprint features\n",
    "        fingerprint = torch.tensor(self.fingerprints[idx], dtype=torch.float32)\n",
    "        \n",
    "        # Combine with property features if available\n",
    "        if self.property_cols:\n",
    "            properties = torch.tensor(self.properties[idx], dtype=torch.float32)\n",
    "            features = torch.cat([fingerprint, properties], dim=0)\n",
    "        else:\n",
    "            features = fingerprint\n",
    "        \n",
    "        # Get target\n",
    "        target = torch.tensor(self.targets[idx], dtype=torch.long)\n",
    "        \n",
    "        return features, target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7aa566",
   "metadata": {},
   "source": [
    "## 3. DEFINE ALL UTILITY FUNCTIONS\n",
    "\n",
    "(Place ALL your function definitions here, in this general order of dependency)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d83261",
   "metadata": {},
   "source": [
    "## 3.1  Utility function to compute class weights, traiing history and confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612ddf96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute class weights for imbalanced data\n",
    "def compute_class_weights(targets):\n",
    "    \"\"\"\n",
    "    Compute class weights inversely proportional to class frequencies.\n",
    "    \n",
    "    Args:\n",
    "        targets: Target labels\n",
    "        \n",
    "    Returns:\n",
    "        Tensor of class weights\n",
    "    \"\"\"\n",
    "    class_counts = np.bincount(targets)\n",
    "    total_samples = len(targets)\n",
    "    \n",
    "    # Compute weights inversely proportional to class frequencies\n",
    "    weights = total_samples / (len(class_counts) * class_counts)\n",
    "    \n",
    "    # Normalize weights\n",
    "    weights = weights / weights.sum()\n",
    "    \n",
    "    return torch.tensor(weights, dtype=torch.float32).to(device)\n",
    "\n",
    "# Plot training history\n",
    "def plot_training_history(history, save_path=None):\n",
    "    \"\"\"\n",
    "    Plot training and validation loss and accuracy.\n",
    "    \n",
    "    Args:\n",
    "        history: Training history dictionary\n",
    "        save_path: Path to save the plot\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Plot loss\n",
    "    ax1.plot(history['train_loss'], label='Train Loss')\n",
    "    ax1.plot(history['val_loss'], label='Validation Loss')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.set_title('Training and Validation Loss')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # Plot accuracy\n",
    "    ax2.plot(history['train_acc'], label='Train Accuracy')\n",
    "    ax2.plot(history['val_acc'], label='Validation Accuracy')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.set_title('Training and Validation Accuracy')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Plot confusion matrix\n",
    "def plot_confusion_matrix(cm, class_names, save_path=None):\n",
    "    \"\"\"\n",
    "    Plot confusion matrix.\n",
    "    \n",
    "    Args:\n",
    "        cm: Confusion matrix\n",
    "        class_names: List of class names\n",
    "        save_path: Path to save the plot\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix')\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0254a829",
   "metadata": {},
   "source": [
    "### Base model with best parameters including activation function support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a7d0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Place this in your \"4. DEFINE ALL UTILITY FUNCTIONS\" section\n",
    "# if you did not have it already.\n",
    "\n",
    "def create_enhanced_model_with_best_params(best_params, input_dim):\n",
    "    \"\"\"\n",
    "    Create enhanced model with best parameters including activation function support.\n",
    "    This function correctly extracts per-layer activation names and parameters\n",
    "    from the best_params dictionary generated by Optuna.\n",
    "    \"\"\"\n",
    "    n_layers = best_params['n_layers']\n",
    "    hidden_dims = [best_params[f'hidden_dim_{i}'] for i in range(n_layers)]\n",
    "    dropout_rates = [best_params[f'dropout_{i}'] for i in range(n_layers)]\n",
    "\n",
    "    # --- Section for extracting per-layer activations ---\n",
    "    activation_names = []\n",
    "    activation_params_list = []\n",
    "\n",
    "    for i in range(n_layers):\n",
    "        current_activation_name = best_params[f'activation_{i}']\n",
    "        activation_names.append(current_activation_name)\n",
    "\n",
    "        current_activation_params = {}\n",
    "        # Check if the specific activation parameter exists for this layer\n",
    "        if current_activation_name == 'PReLU':\n",
    "            if f'prelu_init_{i}' in best_params:\n",
    "                current_activation_params['init'] = best_params[f'prelu_init_{i}']\n",
    "        elif current_activation_name == 'LeakyReLU':\n",
    "            if f'leaky_relu_slope_{i}' in best_params:\n",
    "                current_activation_params['negative_slope'] = best_params[f'leaky_relu_slope_{i}']\n",
    "        elif current_activation_name == 'ELU':\n",
    "            if f'elu_alpha_{i}' in best_params:\n",
    "                current_activation_params['alpha'] = best_params[f'elu_alpha_{i}']\n",
    "        # Add checks for other activation types if they have parameters\n",
    "        \n",
    "        activation_params_list.append(current_activation_params)\n",
    "    # --- End of section for extracting per-layer activations ---\n",
    "\n",
    "    # Instantiate the ToxicityDNN model\n",
    "    # Ensure ToxicityDNN class is defined before this function\n",
    "    model = ToxicityDNN(input_dim, hidden_dims, dropout_rates, activation_names, activation_params_list)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8413f1c",
   "metadata": {},
   "source": [
    "### Data Distribution Visualization Functions\n",
    "\n",
    "Let's add functions to visualize the data distribution for both Tox21 and monomer datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa07a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data_distribution(dataset, dataset_name, class_names=None, save_path=None):\n",
    "    \"\"\"\n",
    "    Plot data distribution for a dataset.\n",
    "    \"\"\"\n",
    "    if class_names is None:\n",
    "        class_names = ['Not_Fulfilled', 'Fulfilled', 'Under_Investigation']\n",
    "    \n",
    "    # Extract targets from dataset\n",
    "    targets = []\n",
    "    for _, target in dataset:\n",
    "        targets.append(target.item())\n",
    "    \n",
    "    # Count classes\n",
    "    unique, counts = np.unique(targets, return_counts=True)\n",
    "    \n",
    "    # Create bar plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    colors = ['skyblue', 'lightcoral', 'lightgreen']\n",
    "    bars = plt.bar([class_names[i] for i in unique], counts, color=[colors[i] for i in unique])\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, count in zip(bars, counts):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(counts)*0.01,\n",
    "                str(count), ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    plt.xlabel('Hazard Criteria')\n",
    "    plt.ylabel('Number of Samples')\n",
    "    plt.title(f'{dataset_name} - Data Distribution')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add percentage labels\n",
    "    total = sum(counts)\n",
    "    for i, (bar, count) in enumerate(zip(bars, counts)):\n",
    "        percentage = (count / total) * 100\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height()/2,\n",
    "                f'{percentage:.1f}%', ha='center', va='center', fontweight='bold', color='white')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\n{dataset_name} Distribution Summary:\")\n",
    "    print(\"-\" * 40)\n",
    "    for i, count in zip(unique, counts):\n",
    "        percentage = (count / total) * 100\n",
    "        print(f\"{class_names[i]:20s}: {count:5d} samples ({percentage:5.1f}%)\")\n",
    "    print(f\"{'Total':20s}: {total:5d} samples\")\n",
    "\n",
    "def plot_tox21_splits_distribution(train_dataset, val_dataset, test_dataset, save_dir=None):\n",
    "    \"\"\"\n",
    "    Plot data distribution for Tox21 train, validation, and test splits.\n",
    "    \"\"\"\n",
    "    class_names = ['Not_Fulfilled', 'Fulfilled', 'Under_Investigation']\n",
    "    \n",
    "    # Create subplots\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    datasets = [train_dataset, val_dataset, test_dataset]\n",
    "    dataset_names = ['Training Set', 'Validation Set', 'Test Set']\n",
    "    colors = ['skyblue', 'lightcoral', 'lightgreen']\n",
    "    \n",
    "    for idx, (dataset, name, ax) in enumerate(zip(datasets, dataset_names, axes)):\n",
    "        # Extract targets\n",
    "        targets = []\n",
    "        for _, target in dataset:\n",
    "            targets.append(target.item())\n",
    "        \n",
    "        # Count classes\n",
    "        unique, counts = np.unique(targets, return_counts=True)\n",
    "        \n",
    "        # Create bar plot\n",
    "        bars = ax.bar([class_names[i] for i in unique], counts, color=[colors[i] for i in unique])\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, count in zip(bars, counts):\n",
    "            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(counts)*0.01,\n",
    "                   str(count), ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "        # Add percentage labels\n",
    "        total = sum(counts)\n",
    "        for i, (bar, count) in enumerate(zip(bars, counts)):\n",
    "            percentage = (count / total) * 100\n",
    "            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height()/2,\n",
    "                   f'{percentage:.1f}%', ha='center', va='center', fontweight='bold', color='white')\n",
    "        \n",
    "        ax.set_xlabel('Hazard Criteria')\n",
    "        ax.set_ylabel('Number of Samples')\n",
    "        ax.set_title(f'Tox21 {name}')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_dir:\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        plt.savefig(os.path.join(save_dir, 'tox21_splits_distribution.png'), dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def plot_monomer_distribution(monomer_dataset, save_path=None):\n",
    "    \"\"\"\n",
    "    Plot data distribution for monomer dataset.\n",
    "    \"\"\"\n",
    "    plot_data_distribution(monomer_dataset, \"Monomer Dataset\", save_path=save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573071b9",
   "metadata": {},
   "source": [
    "### 3.4 Evaluation Function (F1, recall, precision, confusion matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092f2b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced evaluation function\n",
    "def evaluate_model(model, test_loader, class_names=None):\n",
    "    \"\"\"\n",
    "    Evaluate the model on test data with comprehensive metrics.\n",
    "    \n",
    "    Args:\n",
    "        model: PyTorch model\n",
    "        test_loader: DataLoader for test data\n",
    "        class_names: List of class names for classification report\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary of evaluation metrics\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    all_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            \n",
    "            # Get predictions\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            # Collect results\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_targets.extend(targets.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_targets = np.array(all_targets)\n",
    "    all_probs = np.array(all_probs)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(all_targets, all_preds)\n",
    "    f1 = f1_score(all_targets, all_preds, average='weighted')\n",
    "    precision = precision_score(all_targets, all_preds, average='weighted')\n",
    "    recall = recall_score(all_targets, all_preds, average='weighted')\n",
    "    \n",
    "    # Calculate per-class metrics\n",
    "    f1_per_class = f1_score(all_targets, all_preds, average=None)\n",
    "    precision_per_class = precision_score(all_targets, all_preds, average=None)\n",
    "    recall_per_class = recall_score(all_targets, all_preds, average=None)\n",
    "    \n",
    "    # Calculate ROC-AUC (one vs rest)\n",
    "    try:\n",
    "        roc_auc = roc_auc_score(all_targets, all_probs, multi_class='ovr')\n",
    "        # Per-class ROC-AUC\n",
    "        roc_auc_per_class = roc_auc_score(all_targets, all_probs, multi_class='ovr', average=None)\n",
    "    except ValueError:\n",
    "        # Handle case where some classes might be missing\n",
    "        roc_auc = np.nan\n",
    "        roc_auc_per_class = [np.nan] * 3\n",
    "    \n",
    "    # Create confusion matrix\n",
    "    cm = confusion_matrix(all_targets, all_preds)\n",
    "    \n",
    "    # Classification report\n",
    "    if class_names is None:\n",
    "        class_names = ['Not_Fulfilled', 'Fulfilled', 'Under_Investigation']\n",
    "    \n",
    "    report = classification_report(all_targets, all_preds, target_names=class_names, output_dict=True)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'roc_auc': roc_auc,\n",
    "        'f1_per_class': f1_per_class,\n",
    "        'precision_per_class': precision_per_class,\n",
    "        'recall_per_class': recall_per_class,\n",
    "        'roc_auc_per_class': roc_auc_per_class,\n",
    "        'confusion_matrix': cm,\n",
    "        'classification_report': report,\n",
    "        'predictions': all_preds,\n",
    "        'targets': all_targets,\n",
    "        'probabilities': all_probs\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde717ca",
   "metadata": {},
   "source": [
    "### 3.5  ROC AUC Plotting Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432d5efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_roc_curves(y_true, y_prob, class_names, save_path=None, title='Receiver Operating Characteristic (ROC) Curve Fewshot Model'):\n",
    "    \"\"\"\n",
    "    Plots ROC curves (One-vs-Rest and micro-average) for a multi-class problem.\n",
    "    \"\"\"\n",
    "    # Binarize the labels for one-vs-rest ROC calculation\n",
    "    y_true_bin = label_binarize(y_true, classes=range(len(class_names)))\n",
    "    n_classes = y_true_bin.shape[1]\n",
    "    \n",
    "    # Compute ROC curve and ROC area for each class\n",
    "    fpr, tpr, roc_auc = {}, {}, {}\n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_true_bin[:, i], y_prob[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "    # Compute micro-average ROC curve and ROC area\n",
    "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_true_bin.ravel(), y_prob.ravel())\n",
    "    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "    # Plot all ROC curves\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    colors = ['aqua', 'darkorange', 'cornflowerblue']\n",
    "    \n",
    "    for i, color in enumerate(colors):\n",
    "        plt.plot(fpr[i], tpr[i], color=color, lw=2,\n",
    "                 label=f'ROC curve of {class_names[i]} vs rest (area = {roc_auc[i]:0.2f})')\n",
    "\n",
    "    plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "             label=f'micro-average ROC curve (area = {roc_auc[\"micro\"]:.2f})',\n",
    "             color='deeppink', linestyle=':', linewidth=4)\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(title)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300)\n",
    "        \n",
    "    plt.show()\n",
    "\n",
    "    # Print the summary scores\n",
    "    print(\"\\nModel ROC-AUC scores:\")\n",
    "    for i, name in enumerate(class_names):\n",
    "        print(f\"{name} vs rest: {roc_auc[i]:.4f}\")\n",
    "    print(f\"Micro-average: {roc_auc['micro']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4abba87",
   "metadata": {},
   "source": [
    "### 3.6 simplified training function for fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04563b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Paste the utility functions here)\n",
    "\n",
    "# Add this simplified training function for fine-tuning\n",
    "def fine_tune_model(model, train_loader, criterion, optimizer, num_epochs=10):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a83b58",
   "metadata": {},
   "source": [
    "## 4. Load and Prepare Polymer Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246fbe68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the polymer data\n",
    "polymer_df = pd.read_csv(POLYMER_DATA_PATH)\n",
    "print(f\"Polymer dataset loaded with {len(polymer_df)} samples.\")\n",
    "\n",
    "# Extract features and target columns\n",
    "property_cols = [col for col in polymer_df.columns if col.startswith('Property_')]\n",
    "polymer_dataset = PolymerToxicityDataset(polymer_df, 'fingerprints', property_cols, 'Hazard_Criteria_encoded')\n",
    "\n",
    "# Get the input dimension from the first sample\n",
    "sample_features, _ = polymer_dataset[0]\n",
    "input_dim = sample_features.shape[0]\n",
    "\n",
    "# Prepare X and y numpy arrays for splitting\n",
    "X_polymer = []\n",
    "y_polymer = []\n",
    "for features, target in polymer_dataset:\n",
    "    X_polymer.append(features.numpy())\n",
    "    y_polymer.append(target.item())\n",
    "\n",
    "X_polymer = np.array(X_polymer)\n",
    "y_polymer = np.array(y_polymer)\n",
    "\n",
    "print(f\"Polymer data prepared. X shape: {X_polymer.shape}, y shape: {y_polymer.shape}\")\n",
    "print(f\"Polymer dataset loaded with {len(polymer_dataset)} samples.\")\n",
    "print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a080598",
   "metadata": {},
   "source": [
    "### 5.  Data Splitting for Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f846738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- NEW UNIVERSAL DATA SPLITTING CELL ---\n",
    "\n",
    "# Define the sizes for each set based on the new strategy\n",
    "n_total = len(X_polymer)\n",
    "n_test = 0.2  # 20 samples for the final noble test set\n",
    "n_finetune = 0.2 # 20 samples for the initial fine-tuning\n",
    "\n",
    "# Step 1: Split off the \"Noble Test Set\"\n",
    "# This creates a main training pool (76 samples) and the final test set (20 samples).\n",
    "X_train_pool, X_test, y_train_pool, y_test = train_test_split(\n",
    "    X_polymer,\n",
    "    y_polymer,\n",
    "    test_size=n_test,\n",
    "    random_state=RANDOM_SEED,\n",
    "    stratify=y_polymer\n",
    ")\n",
    "\n",
    "# Step 2: Split the training pool into a \"Fine-Tuning Set\" and a \"Cross-Validation Set\"\n",
    "# This takes the 76 samples and splits them into 20 for fine-tuning and 56 for the CV.\n",
    "X_cv, X_finetune, y_cv, y_finetune = train_test_split(\n",
    "    X_train_pool,\n",
    "    y_train_pool,\n",
    "    test_size=n_finetune,\n",
    "    random_state=RANDOM_SEED,\n",
    "    stratify=y_train_pool\n",
    ")\n",
    "\n",
    "# --- Verification ---\n",
    "print(\"Data has been split into three distinct sets:\")\n",
    "print(f\" - Fine-Tuning Set: {len(X_finetune)} samples\")\n",
    "print(f\" - Cross-Validation Set: {len(X_cv)} samples\")\n",
    "print(f\" - Noble Test Set (Final Evaluation): {len(X_test)} samples\")\n",
    "print(f\"Total: {len(X_finetune) + len(X_cv) + len(X_test)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc08b111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Split the polymer data into a 20% fine-tuning set and 80% remaining set for evaluation\n",
    "# X_eval, X_finetune, y_eval, y_finetune = train_test_split(\n",
    "#     X_polymer,\n",
    "#     y_polymer,\n",
    "#     test_size=0.2, # 20% for fine-tuning\n",
    "#     random_state=RANDOM_SEED,\n",
    "#     stratify=y_polymer\n",
    "# )\n",
    "\n",
    "# print(f\"Data split for fine-tuning:\")\n",
    "# print(f\" - Fine-tuning set: {len(X_finetune)} samples\")\n",
    "# print(f\" - Evaluation set (for CV): {len(X_eval)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f22acb",
   "metadata": {},
   "source": [
    "### 6. Load pre-trained Model and Fine-tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa014893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best hyperparameters from the JSON file\n",
    "with open(PARAMS_PATH, 'r') as f:\n",
    "    best_params = json.load(f)\n",
    "\n",
    "# Load the pre-trained base model\n",
    "base_model = create_enhanced_model_with_best_params(best_params, input_dim)\n",
    "base_model.load_state_dict(torch.load(BASE_MODEL_PATH))\n",
    "base_model.to(device)\n",
    "\n",
    "# --- Fine-tuning the model ---\n",
    "print(\"Fine-tuning the pre-trained model on 20% of the polymer data...\")\n",
    "\n",
    "# Create DataLoader for the fine-tuning data\n",
    "finetune_dataset = torch.utils.data.TensorDataset(\n",
    "    torch.tensor(X_finetune, dtype=torch.float32),\n",
    "    torch.tensor(y_finetune, dtype=torch.long)\n",
    ")\n",
    "finetune_loader = DataLoader(finetune_dataset, batch_size=best_params['batch_size'], shuffle=True)\n",
    "\n",
    "# Use a lower learning rate for fine-tuning\n",
    "fine_tune_optimizer = torch.optim.Adam(base_model.parameters(), lr=1e-5) # Lower learning rate\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Fine-tune the model\n",
    "fewshot_model = fine_tune_model(base_model, finetune_loader, criterion, fine_tune_optimizer, num_epochs=50) # Lower epochs\n",
    "\n",
    "print(\"Fine-tuning complete. The 'fewshot_model' is ready for evaluation.\")\n",
    "# --- ADD THIS CODE TO SAVE THE MODEL ---\n",
    "save_path = os.path.join(RESULTS_DIR, \"fewshot_model.pt\")\n",
    "torch.save(fewshot_model.state_dict(), save_path)\n",
    "print(f\"Few-shot model successfully saved to: {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e1aba6",
   "metadata": {},
   "source": [
    "### 7. 5-Fold Cross-Validation for Few-Shot Evaluation\n",
    "\n",
    "Use the X_eval and y_eval data (the remaining 77 data points).\n",
    "\n",
    " In each fold, we will further split the data into 80% for training and 20% for testing.\n",
    "\n",
    "The fewshot_model will be trained on the 80% split and tested on the 20% split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4757b532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5-Fold Cross-Validation on the CV Set (56 samples) ---\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_SEED)\n",
    "summary_metrics = []\n",
    "all_fold_preds = np.array([])\n",
    "all_fold_targets = np.array([])\n",
    "all_fold_probs = None\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Starting 5-Fold Cross-Validation on the CV Set\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# The CV loop now runs on X_cv and y_cv\n",
    "for fold, (train_idx, test_idx) in enumerate(skf.split(X_cv, y_cv)):\n",
    "    print(f\"\\nFold {fold + 1}/5\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    # Use the CV data for this fold\n",
    "    X_train_fold, X_test_fold = X_cv[train_idx], X_cv[test_idx]\n",
    "    y_train_fold, y_test_fold = y_cv[train_idx], y_cv[test_idx]\n",
    "\n",
    "    print(f\"Train samples for this fold: {len(X_train_fold)}\")\n",
    "    print(f\"Test samples for this fold: {len(X_test_fold)}\")\n",
    "    \n",
    "    # Use a fresh, deep copy of the fine-tuned model for each fold to prevent data leakage\n",
    "    model_for_fold = copy.deepcopy(fewshot_model)\n",
    "    \n",
    "    # Create DataLoaders\n",
    "    train_dataset_fold = torch.utils.data.TensorDataset(torch.tensor(X_train_fold, dtype=torch.float32), torch.tensor(y_train_fold, dtype=torch.long))\n",
    "    train_loader_fold = DataLoader(train_dataset_fold, batch_size=best_params['batch_size'], shuffle=True)\n",
    "    test_dataset_fold = torch.utils.data.TensorDataset(torch.tensor(X_test_fold, dtype=torch.float32), torch.tensor(y_test_fold, dtype=torch.long))\n",
    "    test_loader_fold = DataLoader(test_dataset_fold, batch_size=best_params['batch_size'], shuffle=False)\n",
    "\n",
    "    # Train the model copy on this fold's training data\n",
    "    optimizer_fold = torch.optim.Adam(model_for_fold.parameters(), lr=1e-5)\n",
    "    criterion_fold = nn.CrossEntropyLoss()\n",
    "    trained_fold_model = fine_tune_model(model_for_fold, train_loader_fold, criterion_fold, optimizer_fold, num_epochs=50)\n",
    "\n",
    "    # Evaluate on the test set of the fold\n",
    "    fold_metrics = evaluate_model(trained_fold_model, test_loader_fold, class_names)\n",
    "\n",
    "    # Print results and aggregate for summary\n",
    "    print(f\"Accuracy:  {fold_metrics['accuracy']:.4f}\")\n",
    "    print(f\"F1 Score:  {fold_metrics['f1']:.4f}\")\n",
    "    print(f\"ROC-AUC:   {fold_metrics['roc_auc']:.4f}\")\n",
    "    summary_metrics.append(fold_metrics)\n",
    "    all_fold_preds = np.concatenate([all_fold_preds, fold_metrics['predictions']])\n",
    "    all_fold_targets = np.concatenate([all_fold_targets, fold_metrics['targets']])\n",
    "    if all_fold_probs is None:\n",
    "        all_fold_probs = fold_metrics['probabilities']\n",
    "    else:\n",
    "        all_fold_probs = np.vstack([all_fold_probs, fold_metrics['probabilities']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad67c8a1",
   "metadata": {},
   "source": [
    "### 8. Summary Statistics and Final Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8ca6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Summary statistics from the 5-Fold CV on the development set ---\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"MODEL ROBUSTNESS: SUMMARY STATISTICS ACROSS CV FOLDS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Extract each metric from the list of dictionaries\n",
    "accuracies = [m['accuracy'] for m in summary_metrics]\n",
    "precisions = [m['precision'] for m in summary_metrics]\n",
    "recalls = [m['recall'] for m in summary_metrics]\n",
    "f1s = [m['f1'] for m in summary_metrics]\n",
    "roc_aucs = [m['roc_auc'] for m in summary_metrics]\n",
    "\n",
    "# Calculate and print the stats in the desired format\n",
    "print(f\"ACCURACY  : {np.mean(accuracies):.4f} ¬± {np.std(accuracies):.4f} (min: {np.min(accuracies):.4f}, max: {np.max(accuracies):.4f})\")\n",
    "print(f\"PRECISION : {np.mean(precisions):.4f} ¬± {np.std(precisions):.4f} (min: {np.min(precisions):.4f}, max: {np.max(precisions):.4f})\")\n",
    "print(f\"RECALL    : {np.mean(recalls):.4f} ¬± {np.std(recalls):.4f} (min: {np.min(recalls):.4f}, max: {np.max(recalls):.4f})\")\n",
    "print(f\"F1        : {np.mean(f1s):.4f} ¬± {np.std(f1s):.4f} (min: {np.min(f1s):.4f}, max: {np.max(f1s):.4f})\")\n",
    "print(f\"ROC_AUC   : {np.mean(roc_aucs):.4f} ¬± {np.std(roc_aucs):.4f} (min: {np.min(roc_aucs):.4f}, max: {np.max(roc_aucs):.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bc0a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Train Final Model and Evaluate on the Noble Test Set ---\n",
    "\n",
    "# Train the final model on the ENTIRE CV set to use all available training data\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Training final model on the entire CV set (56 samples)...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create a fresh copy of the initial fine-tuned model\n",
    "final_model = copy.deepcopy(fewshot_model)\n",
    "\n",
    "# Create a DataLoader for the full CV dataset\n",
    "cv_dataset = torch.utils.data.TensorDataset(torch.tensor(X_cv, dtype=torch.float32), torch.tensor(y_cv, dtype=torch.long))\n",
    "cv_loader = DataLoader(cv_dataset, batch_size=best_params['batch_size'], shuffle=True)\n",
    "\n",
    "# Train the model on the full CV set\n",
    "final_optimizer = torch.optim.Adam(final_model.parameters(), lr=1e-5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "trained_final_model = fine_tune_model(final_model, cv_loader, criterion, final_optimizer, num_epochs=50)\n",
    "\n",
    "print(\"Final model training complete.\")\n",
    "\n",
    "# --- Final Evaluation Report on the NOBLE TEST SET ---\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"FINAL PERFORMANCE ON UNSEEN NOBLE TEST SET (20 SAMPLES)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create a DataLoader for the noble test set\n",
    "test_dataset_noble = torch.utils.data.TensorDataset(torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.long))\n",
    "test_loader_noble = DataLoader(test_dataset_noble, batch_size=best_params['batch_size'], shuffle=False)\n",
    "\n",
    "# Evaluate the final model on the noble test set\n",
    "final_metrics = evaluate_model(trained_final_model, test_loader_noble, class_names)\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\nDetailed Classification Report for Final Model:\")\n",
    "print(\"-\" * 60)\n",
    "print(classification_report(final_metrics['targets'], final_metrics['predictions'], target_names=class_names))\n",
    "\n",
    "# Confusion Matrix\n",
    "print(\"\\nConfusion Matrix for Final Model:\")\n",
    "cm = confusion_matrix(final_metrics['targets'], final_metrics['predictions'])\n",
    "plot_confusion_matrix(cm, class_names, save_path=os.path.join(RESULTS_DIR, \"fewshot_confusion_matrix.png\"))\n",
    "\n",
    "# Per-class Accuracy\n",
    "print(\"\\nPer-class Accuracy for Final Model:\")\n",
    "print(\"-\" * 40)\n",
    "for i, name in enumerate(class_names):\n",
    "    true_samples_for_class = cm[i].sum()\n",
    "    if true_samples_for_class > 0:\n",
    "        class_accuracy = cm[i, i] / true_samples_for_class\n",
    "        print(f\"{name:25s}: {class_accuracy:.4f} ({cm[i, i]}/{true_samples_for_class})\")\n",
    "    else:\n",
    "        print(f\"{name:25s}: No test samples in this class\")\n",
    "overall_accuracy = np.trace(cm) / np.sum(cm)\n",
    "print(f\"\\nOverall Accuracy: {overall_accuracy:.4f}\")\n",
    "\n",
    "\n",
    "# ROC AUC Curve\n",
    "print(\"\\nROC AUC Curve (One vs Rest) for Final Model:\")\n",
    "plot_roc_curves(final_metrics['targets'], final_metrics['probabilities'], class_names, save_path=os.path.join(RESULTS_DIR, \"final_roc_curve.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d2a2ea",
   "metadata": {},
   "source": [
    "How We Get a 77-Point Confusion Matrix\n",
    "Think of your 77 data points as a full deck of cards. 5-fold cross-validation is a clever way to ensure that you test your model on every single card in the deck exactly once.\n",
    "\n",
    "Here‚Äôs how it works:\n",
    "\n",
    "Fold 1: The model trains on 61 cards and is tested on the first 16 cards. We save the predictions for these 16 cards.\n",
    "\n",
    "Fold 2: The model trains on a different set of 61 cards and is tested on the next 16 cards (cards 17-32). We save the predictions for this new set.\n",
    "\n",
    "Fold 3: We test on the next 16 cards (33-48) and save those predictions.\n",
    "\n",
    "Fold 4: We test on the next 16 cards (49-64) and save those predictions.\n",
    "\n",
    "Fold 5: We test on the final 13 cards (65-77) and save those predictions.\n",
    "\n",
    "At the end of the 5 folds, you haven't tested on one small set of 16. Instead, you have collected the test predictions for all 77 unique data points.\n",
    "\n",
    "The final confusion matrix is created by comparing the true labels of all 77 points to the collected test predictions for all 77 points.\n",
    "\n",
    "Why This Is the Correct and Best Method\n",
    "It's 100% Test Data: Every prediction used in the final 77-point matrix was generated when that data point was in a test set. The model had never seen that specific data point during its training phase in that fold.\n",
    "\n",
    "More Reliable and Stable: A confusion matrix on only 16 samples is highly unstable. If the model gets just one sample wrong, the accuracy can swing wildly (e.g., from 93.7% to 87.5%). A matrix built on all 77 test results gives a much more trustworthy and stable picture of the model's true performance.\n",
    "\n",
    "It Evaluates the Entire Dataset: This method gives you a single, comprehensive report on how your model performs across your entire evaluation set, not just one small, random slice of it.\n",
    "\n",
    "So, to answer your direct question:\n",
    "I want to show a final confusion matrix with 16 data points but aggregate of all folds on test data?\n",
    "\n",
    "This is not possible because you can't \"aggregate\" five different sets of 16 predictions into a single 16-point matrix. The correct way to aggregate them is to combine them, which gives you the full set of 77 predictions.\n",
    "\n",
    "In short: Your final 77-point confusion matrix is the correct and most robust way to report the overall test performance of your 5-fold cross-validation. The individual 16-point matrices you can generate are for diagnosing performance fold-by-fold, but the big one is your final result."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
