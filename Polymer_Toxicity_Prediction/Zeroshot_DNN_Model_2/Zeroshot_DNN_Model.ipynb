{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zeroshot Polymer DNN Model\n",
    "\n",
    "This notebook implements a zeroshot Deep Neural Network (DNN) model for polymer toxicity prediction using PyTorch and Optuna for hyperparameter optimization. It leverages transfer learning from the Tox21 dataset to predict polymer toxicity, specifically using the last 28 entries as the polymer/monomer dataset for Zero-shot learning and validation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. GPU Avaibilty Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import torch\n",
    "\n",
    "def get_filtered_free_gpus(allowed_gpus={0, 2, 3}):\n",
    "    \"\"\"Return (gpu_id, free_mem_MB) from allowed_gpus, sorted by free memory.\"\"\"\n",
    "    try:\n",
    "        result = subprocess.check_output(\n",
    "            ['nvidia-smi', '--query-gpu=index,memory.free', '--format=csv,noheader,nounits'],\n",
    "            encoding='utf-8'\n",
    "        )\n",
    "        gpu_info = []\n",
    "        for line in result.strip().split('\\n'):\n",
    "            gpu_id, mem_free = line.strip().split(',')\n",
    "            gpu_id = int(gpu_id)\n",
    "            mem_free = int(mem_free)\n",
    "            if gpu_id in allowed_gpus:\n",
    "                gpu_info.append((gpu_id, mem_free))\n",
    "        return sorted(gpu_info, key=lambda x: x[1], reverse=True)\n",
    "    except Exception as e:\n",
    "        print(\"‚ö†Ô∏è Could not query GPU info:\", e)\n",
    "        return []\n",
    "\n",
    "# Step 1: Get preferred GPU (among 0, 2, 3)\n",
    "available_gpus = get_filtered_free_gpus()\n",
    "\n",
    "if available_gpus:\n",
    "    selected_system_gpu = available_gpus[0][0]\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(selected_system_gpu)\n",
    "    print(f\"üß† Selected system GPU ID: {selected_system_gpu} with {available_gpus[0][1]} MB free memory.\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No preferred GPUs available. Using CPU.\")\n",
    "\n",
    "# Step 2: PyTorch setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    current_device = torch.cuda.current_device()\n",
    "    visible_device_name = torch.cuda.get_device_name(current_device)\n",
    "    visible_ids = os.environ[\"CUDA_VISIBLE_DEVICES\"].split(\",\")\n",
    "    real_system_gpu_id = visible_ids[current_device]\n",
    "\n",
    "    print(\"‚úÖ GPU is available!\")\n",
    "    print(f\"üñ•Ô∏è Visible PyTorch Device: cuda:{current_device}\")\n",
    "    print(f\"üß≠ Actual system GPU ID: {real_system_gpu_id}\")\n",
    "    print(f\"üìü GPU Name: {visible_device_name}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è GPU not available. Using CPU.\")\n",
    "\n",
    "print(f\"Model will run on: {device}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(\"My PID:\", os.getpid())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. # IMPORTS AND GLOBAL CONFIGURATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, precision_score, recall_score,\n",
    "    roc_auc_score, confusion_matrix, classification_report\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import ast\n",
    "from tqdm.notebook import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "\n",
    "print(\"Libraries imported and random seed set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  CONFIGURATION AND PATHS FOR ZERO-SHOT ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. CONFIGURATION AND PATHS FOR ZERO-SHOT ---\n",
    "\n",
    "# Paths provided by you\n",
    "BASE_MODEL_PATH = \"/home/sunil/am2/poetry-demo/Polytox_Matser_Thesis/Few_shot_learning_with_validation_data_Monomers/Zeroshot_DNN_Model_2/models/base_model.pt\"\n",
    "PARAMS_PATH = \"/home/sunil/am2/poetry-demo/Polytox_Matser_Thesis/Few_shot_learning_with_validation_data_Monomers/Zeroshot_DNN_Model_2/optuna_results/best_params.json\"\n",
    "POLYMER_DATA_PATH = \"/home/sunil/am2/poetry-demo/Polytox_Matser_Thesis/Few_shot_learning_with_validation_data_Monomers/Zeroshot_DNN_Model_2/polymer_DNN_encoded_final_data.csv\"\n",
    "\n",
    "# Directory to save new results\n",
    "RESULTS_DIR = \"./results_zeroshot\"\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "# Define class names for plotting and reports\n",
    "class_names = ['Not_Fulfilled', 'Fulfilled', 'Under_Investigation']\n",
    "\n",
    "print(\"Configuration for Zero-Shot evaluation is set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. DEFINE YOUR DNN MODEL CLASS(ToxiciityDNN CLASS DEFINITION )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ToxicityDNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, dropout_rates, activation_names, activation_params_list):\n",
    "        super(ToxicityDNN, self).__init__()\n",
    "        self.n_layers = len(hidden_dims)\n",
    "        self.dropout_rates = dropout_rates\n",
    "        self.activation_names = activation_names # Store for potential later inspection if needed\n",
    "        self.activation_params_list = activation_params_list # Store for potential later inspection if needed\n",
    "\n",
    "        self.activation_modules = nn.ModuleList() # Use ModuleList for activations\n",
    "\n",
    "        # Combine Linear and Dropout layers into a single ModuleList for simpler iteration\n",
    "        self.linear_layers = nn.ModuleList()\n",
    "        self.dropout_layers = nn.ModuleList()\n",
    "\n",
    "        in_dim = input_dim\n",
    "\n",
    "        for i in range(self.n_layers):\n",
    "            out_dim = hidden_dims[i]\n",
    "            self.linear_layers.append(nn.Linear(in_dim, out_dim)) # Append linear layer\n",
    "\n",
    "            # Create activation module for the current layer\n",
    "            activation_name = activation_names[i]\n",
    "            # Safely get params for the current layer, default to empty dict if index out of bounds\n",
    "            activation_params = activation_params_list[i] if i < len(activation_params_list) else {}\n",
    "\n",
    "            if activation_name == 'ReLU':\n",
    "                self.activation_modules.append(nn.ReLU())\n",
    "            elif activation_name == 'PReLU':\n",
    "                # num_parameters should match the input feature dimension if you want one param per feature\n",
    "                # or just 1 if shared across all features. For simplicity, we'll keep 1 as in your original\n",
    "                self.activation_modules.append(nn.PReLU(num_parameters=1, init=activation_params.get('init', 0.25)))\n",
    "            elif activation_name == 'SiLU':\n",
    "                self.activation_modules.append(nn.SiLU())\n",
    "            elif activation_name == 'LeakyReLU':\n",
    "                self.activation_modules.append(nn.LeakyReLU(negative_slope=activation_params.get('negative_slope', 0.01)))\n",
    "            elif activation_name == 'ELU':\n",
    "                self.activation_modules.append(nn.ELU(alpha=activation_params.get('alpha', 1.0)))\n",
    "            elif activation_name == 'GELU':\n",
    "                self.activation_modules.append(nn.GELU())\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown activation function: {activation_name}\")\n",
    "\n",
    "            self.dropout_layers.append(nn.Dropout(dropout_rates[i])) # Append dropout layer\n",
    "            in_dim = out_dim\n",
    "\n",
    "        # Output layer\n",
    "        self.output_layer = nn.Linear(in_dim, 3) # Assuming 3 output classes\n",
    "\n",
    "    # ONLY ONE FORWARD METHOD\n",
    "    def forward(self, x):\n",
    "        for i in range(self.n_layers):\n",
    "            x = self.linear_layers[i](x)      # Apply Linear layer\n",
    "            x = self.activation_modules[i](x) # Apply specific activation for this layer\n",
    "            x = self.dropout_layers[i](x)     # Apply Dropout layer\n",
    "\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "    # Remove or modify get_activation_info if you need it to reflect per-layer info\n",
    "    # For example, to get info for all layers:\n",
    "    def get_all_activation_info(self):\n",
    "        \"\"\"\n",
    "        Get information about activation functions used in all layers.\n",
    "        \"\"\"\n",
    "        info = []\n",
    "        for i in range(self.n_layers):\n",
    "            info.append({\n",
    "                'name': self.activation_names[i],\n",
    "                'params': self.activation_params_list[i]\n",
    "            })\n",
    "        return info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. DEFINE  DATASET CLASS (e.g., PolymerToxicityDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolymerToxicityDataset(Dataset):\n",
    "    def __init__(self, dataframe, fingerprint_col='fingerprints', property_cols=None, target_col='Hazard_Criteria_encoded'):\n",
    "        \"\"\"\n",
    "        Initialize the dataset.\n",
    "        \n",
    "        Args:\n",
    "            dataframe: Pandas DataFrame containing the data\n",
    "            fingerprint_col: Column name for fingerprint vectors\n",
    "            property_cols: List of property column names (one-hot encoded)\n",
    "            target_col: Column name for the target variable\n",
    "        \"\"\"\n",
    "        self.dataframe = dataframe\n",
    "        self.fingerprint_col = fingerprint_col\n",
    "        self.property_cols = property_cols if property_cols else []\n",
    "        self.target_col = target_col\n",
    "        \n",
    "        # Convert string representation of lists to actual lists for fingerprints\n",
    "        self.fingerprints = self.dataframe[fingerprint_col].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x).tolist()\n",
    "        \n",
    "        # Get property features if available\n",
    "        if self.property_cols:\n",
    "            self.properties = self.dataframe[self.property_cols].values\n",
    "        \n",
    "        # Get targets\n",
    "        self.targets = self.dataframe[target_col].values\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get fingerprint features\n",
    "        fingerprint = torch.tensor(self.fingerprints[idx], dtype=torch.float32)\n",
    "        \n",
    "        # Combine with property features if available\n",
    "        if self.property_cols:\n",
    "            properties = torch.tensor(self.properties[idx], dtype=torch.float32)\n",
    "            features = torch.cat([fingerprint, properties], dim=0)\n",
    "        else:\n",
    "            features = fingerprint\n",
    "        \n",
    "        # Get target\n",
    "        target = torch.tensor(self.targets[idx], dtype=torch.long)\n",
    "        \n",
    "        return features, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. DEFINE ALL UTILITY FUNCTIONS\n",
    "\n",
    "(Place ALL your function definitions here, in this general order of dependency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1  Utility function to compute class weights, traiing history and confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute class weights for imbalanced data\n",
    "def compute_class_weights(targets):\n",
    "    \"\"\"\n",
    "    Compute class weights inversely proportional to class frequencies.\n",
    "    \n",
    "    Args:\n",
    "        targets: Target labels\n",
    "        \n",
    "    Returns:\n",
    "        Tensor of class weights\n",
    "    \"\"\"\n",
    "    class_counts = np.bincount(targets)\n",
    "    total_samples = len(targets)\n",
    "    \n",
    "    # Compute weights inversely proportional to class frequencies\n",
    "    weights = total_samples / (len(class_counts) * class_counts)\n",
    "    \n",
    "    # Normalize weights\n",
    "    weights = weights / weights.sum()\n",
    "    \n",
    "    return torch.tensor(weights, dtype=torch.float32).to(device)\n",
    "\n",
    "# Plot training history\n",
    "def plot_training_history(history, save_path=None):\n",
    "    \"\"\"\n",
    "    Plot training and validation loss and accuracy.\n",
    "    \n",
    "    Args:\n",
    "        history: Training history dictionary\n",
    "        save_path: Path to save the plot\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Plot loss\n",
    "    ax1.plot(history['train_loss'], label='Train Loss')\n",
    "    ax1.plot(history['val_loss'], label='Validation Loss')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.set_title('Training and Validation Loss')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # Plot accuracy\n",
    "    ax2.plot(history['train_acc'], label='Train Accuracy')\n",
    "    ax2.plot(history['val_acc'], label='Validation Accuracy')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.set_title('Training and Validation Accuracy')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Plot confusion matrix\n",
    "def plot_confusion_matrix(cm, class_names, save_path=None):\n",
    "    \"\"\"\n",
    "    Plot confusion matrix.\n",
    "    \n",
    "    Args:\n",
    "        cm: Confusion matrix\n",
    "        class_names: List of class names\n",
    "        save_path: Path to save the plot\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix')\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base model with best parameters including activation function support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Place this in your \"4. DEFINE ALL UTILITY FUNCTIONS\" section\n",
    "# if you did not have it already.\n",
    "\n",
    "def create_enhanced_model_with_best_params(best_params, input_dim):\n",
    "    \"\"\"\n",
    "    Create enhanced model with best parameters including activation function support.\n",
    "    This function correctly extracts per-layer activation names and parameters\n",
    "    from the best_params dictionary generated by Optuna.\n",
    "    \"\"\"\n",
    "    n_layers = best_params['n_layers']\n",
    "    hidden_dims = [best_params[f'hidden_dim_{i}'] for i in range(n_layers)]\n",
    "    dropout_rates = [best_params[f'dropout_{i}'] for i in range(n_layers)]\n",
    "\n",
    "    # --- Section for extracting per-layer activations ---\n",
    "    activation_names = []\n",
    "    activation_params_list = []\n",
    "\n",
    "    for i in range(n_layers):\n",
    "        current_activation_name = best_params[f'activation_{i}']\n",
    "        activation_names.append(current_activation_name)\n",
    "\n",
    "        current_activation_params = {}\n",
    "        # Check if the specific activation parameter exists for this layer\n",
    "        if current_activation_name == 'PReLU':\n",
    "            if f'prelu_init_{i}' in best_params:\n",
    "                current_activation_params['init'] = best_params[f'prelu_init_{i}']\n",
    "        elif current_activation_name == 'LeakyReLU':\n",
    "            if f'leaky_relu_slope_{i}' in best_params:\n",
    "                current_activation_params['negative_slope'] = best_params[f'leaky_relu_slope_{i}']\n",
    "        elif current_activation_name == 'ELU':\n",
    "            if f'elu_alpha_{i}' in best_params:\n",
    "                current_activation_params['alpha'] = best_params[f'elu_alpha_{i}']\n",
    "        # Add checks for other activation types if they have parameters\n",
    "        \n",
    "        activation_params_list.append(current_activation_params)\n",
    "    # --- End of section for extracting per-layer activations ---\n",
    "\n",
    "    # Instantiate the ToxicityDNN model\n",
    "    # Ensure ToxicityDNN class is defined before this function\n",
    "    model = ToxicityDNN(input_dim, hidden_dims, dropout_rates, activation_names, activation_params_list)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 utility functiion for early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"\n",
    "    Early stopping to stop training when validation loss doesn't improve.\n",
    "    \"\"\"\n",
    "    def __init__(self, patience=5, min_delta=0.01, restore_best_weights=True, verbose=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience: Number of epochs to wait before stopping\n",
    "            min_delta: Minimum change to qualify as an improvement\n",
    "            restore_best_weights: Whether to restore model weights from the best epoch\n",
    "            verbose: Whether to print early stopping messages\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.restore_best_weights = restore_best_weights\n",
    "        self.verbose = verbose\n",
    "        \n",
    "        self.best_loss = None\n",
    "        self.counter = 0\n",
    "        self.best_weights = None\n",
    "        self.early_stop = False\n",
    "        \n",
    "    def __call__(self, val_loss, model):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "            self.save_checkpoint(model)\n",
    "        elif val_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "            self.save_checkpoint(model)\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            \n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "                if self.restore_best_weights:\n",
    "                    self.restore_checkpoint(model)\n",
    "                    if self.verbose:\n",
    "                        print('Restoring model weights from the end of the best epoch.')\n",
    "    \n",
    "    def save_checkpoint(self, model):\n",
    "        \"\"\"Save model when validation loss decreases.\"\"\"\n",
    "        if self.restore_best_weights:\n",
    "            self.best_weights = model.state_dict().copy()\n",
    "    \n",
    "    def restore_checkpoint(self, model):\n",
    "        \"\"\"Restore model to the best checkpoint.\"\"\"\n",
    "        if self.best_weights is not None:\n",
    "            model.load_state_dict(self.best_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 3.3  Training function with early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced training function with early stopping\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=100, \n",
    "                early_stopping=None, scheduler=None, verbose=True):\n",
    "    \"\"\"\n",
    "    Train the model with optional early stopping.\n",
    "    \n",
    "    Args:\n",
    "        model: PyTorch model\n",
    "        train_loader: DataLoader for training data\n",
    "        val_loader: DataLoader for validation data\n",
    "        criterion: Loss function\n",
    "        optimizer: Optimizer\n",
    "        num_epochs: Maximum number of epochs\n",
    "        early_stopping: EarlyStopping object (optional)\n",
    "        scheduler: Learning rate scheduler (optional)\n",
    "        verbose: Whether to print progress\n",
    "        \n",
    "    Returns:\n",
    "        Trained model and training history\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    \n",
    "    # Initialize history\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_acc': []\n",
    "    }\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        for inputs, targets in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\", leave=False):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "\n",
    "            # üîß OVERFITTING FIX: Add gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Statistics\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            train_total += targets.size(0)\n",
    "            train_correct += (predicted == targets).sum().item()\n",
    "        \n",
    "        train_loss = train_loss / len(train_loader.dataset)\n",
    "        train_acc = train_correct / train_total\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Val]\", leave=False):\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                \n",
    "                # Statistics\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                val_total += targets.size(0)\n",
    "                val_correct += (predicted == targets).sum().item()\n",
    "        \n",
    "        val_loss = val_loss / len(val_loader.dataset)\n",
    "        val_acc = val_correct / val_total\n",
    "        \n",
    "        # # üîß OVERFITTING FIX: Update learning rate scheduler\n",
    "        if scheduler is not None:\n",
    "            if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "                scheduler.step(val_loss)\n",
    "            else:\n",
    "                scheduler.step()\n",
    "            if verbose and scheduler is not None and not isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "                print(f\"Learning Rate after Epoch {epoch+1}: {scheduler.get_last_lr()}\")\n",
    "\n",
    "        \n",
    "        # Update history\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        \n",
    "        # Print progress\n",
    "        if verbose:\n",
    "            print(f'Epoch {epoch+1}/{num_epochs} | '\n",
    "                  f'Train Loss: {train_loss:.4f} | '\n",
    "                  f'Val Loss: {val_loss:.4f} | '\n",
    "                  f'Train Acc: {train_acc:.4f} | '\n",
    "                  f'Val Acc: {val_acc:.4f}')\n",
    "        \n",
    "        # Early stopping check\n",
    "        if early_stopping is not None:\n",
    "            early_stopping(val_loss, model)\n",
    "            if early_stopping.early_stop:\n",
    "                if verbose:\n",
    "                    print(f'Early stopping at epoch {epoch+1}')\n",
    "                break\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Distribution Visualization Functions\n",
    "\n",
    "Let's add functions to visualize the data distribution for both Tox21 and monomer datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data_distribution(dataset, dataset_name, class_names=None, save_path=None):\n",
    "    \"\"\"\n",
    "    Plot data distribution for a dataset.\n",
    "    \"\"\"\n",
    "    if class_names is None:\n",
    "        class_names = ['Not_Fulfilled', 'Fulfilled', 'Under_Investigation']\n",
    "    \n",
    "    # Extract targets from dataset\n",
    "    targets = []\n",
    "    for _, target in dataset:\n",
    "        targets.append(target.item())\n",
    "    \n",
    "    # Count classes\n",
    "    unique, counts = np.unique(targets, return_counts=True)\n",
    "    \n",
    "    # Create bar plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    colors = ['skyblue', 'lightcoral', 'lightgreen']\n",
    "    bars = plt.bar([class_names[i] for i in unique], counts, color=[colors[i] for i in unique])\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, count in zip(bars, counts):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(counts)*0.01,\n",
    "                str(count), ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    plt.xlabel('Hazard Criteria')\n",
    "    plt.ylabel('Number of Samples')\n",
    "    plt.title(f'{dataset_name} - Data Distribution')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add percentage labels\n",
    "    total = sum(counts)\n",
    "    for i, (bar, count) in enumerate(zip(bars, counts)):\n",
    "        percentage = (count / total) * 100\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height()/2,\n",
    "                f'{percentage:.1f}%', ha='center', va='center', fontweight='bold', color='white')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\n{dataset_name} Distribution Summary:\")\n",
    "    print(\"-\" * 40)\n",
    "    for i, count in zip(unique, counts):\n",
    "        percentage = (count / total) * 100\n",
    "        print(f\"{class_names[i]:20s}: {count:5d} samples ({percentage:5.1f}%)\")\n",
    "    print(f\"{'Total':20s}: {total:5d} samples\")\n",
    "\n",
    "def plot_tox21_splits_distribution(train_dataset, val_dataset, test_dataset, save_dir=None):\n",
    "    \"\"\"\n",
    "    Plot data distribution for Tox21 train, validation, and test splits.\n",
    "    \"\"\"\n",
    "    class_names = ['Not_Fulfilled', 'Fulfilled', 'Under_Investigation']\n",
    "    \n",
    "    # Create subplots\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    datasets = [train_dataset, val_dataset, test_dataset]\n",
    "    dataset_names = ['Training Set', 'Validation Set', 'Test Set']\n",
    "    colors = ['skyblue', 'lightcoral', 'lightgreen']\n",
    "    \n",
    "    for idx, (dataset, name, ax) in enumerate(zip(datasets, dataset_names, axes)):\n",
    "        # Extract targets\n",
    "        targets = []\n",
    "        for _, target in dataset:\n",
    "            targets.append(target.item())\n",
    "        \n",
    "        # Count classes\n",
    "        unique, counts = np.unique(targets, return_counts=True)\n",
    "        \n",
    "        # Create bar plot\n",
    "        bars = ax.bar([class_names[i] for i in unique], counts, color=[colors[i] for i in unique])\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, count in zip(bars, counts):\n",
    "            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(counts)*0.01,\n",
    "                   str(count), ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "        # Add percentage labels\n",
    "        total = sum(counts)\n",
    "        for i, (bar, count) in enumerate(zip(bars, counts)):\n",
    "            percentage = (count / total) * 100\n",
    "            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height()/2,\n",
    "                   f'{percentage:.1f}%', ha='center', va='center', fontweight='bold', color='white')\n",
    "        \n",
    "        ax.set_xlabel('Hazard Criteria')\n",
    "        ax.set_ylabel('Number of Samples')\n",
    "        ax.set_title(f'Tox21 {name}')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_dir:\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        plt.savefig(os.path.join(save_dir, 'tox21_splits_distribution.png'), dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def plot_monomer_distribution(monomer_dataset, save_path=None):\n",
    "    \"\"\"\n",
    "    Plot data distribution for monomer dataset.\n",
    "    \"\"\"\n",
    "    plot_data_distribution(monomer_dataset, \"Monomer Dataset\", save_path=save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Evaluation Function (F1, recall, precision, confusion matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced evaluation function\n",
    "def evaluate_model(model, test_loader, class_names=None):\n",
    "    \"\"\"\n",
    "    Evaluate the model on test data with comprehensive metrics.\n",
    "    \n",
    "    Args:\n",
    "        model: PyTorch model\n",
    "        test_loader: DataLoader for test data\n",
    "        class_names: List of class names for classification report\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary of evaluation metrics\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    all_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            \n",
    "            # Get predictions\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            # Collect results\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_targets.extend(targets.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_targets = np.array(all_targets)\n",
    "    all_probs = np.array(all_probs)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(all_targets, all_preds)\n",
    "    f1 = f1_score(all_targets, all_preds, average='weighted')\n",
    "    precision = precision_score(all_targets, all_preds, average='weighted')\n",
    "    recall = recall_score(all_targets, all_preds, average='weighted')\n",
    "    \n",
    "    # Calculate per-class metrics\n",
    "    f1_per_class = f1_score(all_targets, all_preds, average=None)\n",
    "    precision_per_class = precision_score(all_targets, all_preds, average=None)\n",
    "    recall_per_class = recall_score(all_targets, all_preds, average=None)\n",
    "    \n",
    "    # Calculate ROC-AUC (one vs rest)\n",
    "    try:\n",
    "        roc_auc = roc_auc_score(all_targets, all_probs, multi_class='ovr')\n",
    "        # Per-class ROC-AUC\n",
    "        roc_auc_per_class = roc_auc_score(all_targets, all_probs, multi_class='ovr', average=None)\n",
    "    except ValueError:\n",
    "        # Handle case where some classes might be missing\n",
    "        roc_auc = np.nan\n",
    "        roc_auc_per_class = [np.nan] * 3\n",
    "    \n",
    "    # Create confusion matrix\n",
    "    cm = confusion_matrix(all_targets, all_preds)\n",
    "    \n",
    "    # Classification report\n",
    "    if class_names is None:\n",
    "        class_names = ['Not_Fulfilled', 'Fulfilled', 'Under_Investigation']\n",
    "    \n",
    "    report = classification_report(all_targets, all_preds, target_names=class_names, output_dict=True)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'roc_auc': roc_auc,\n",
    "        'f1_per_class': f1_per_class,\n",
    "        'precision_per_class': precision_per_class,\n",
    "        'recall_per_class': recall_per_class,\n",
    "        'roc_auc_per_class': roc_auc_per_class,\n",
    "        'confusion_matrix': cm,\n",
    "        'classification_report': report,\n",
    "        'predictions': all_preds,\n",
    "        'targets': all_targets,\n",
    "        'probabilities': all_probs\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5  ROC AUC Plotting Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_roc_curves(y_true, y_prob, class_names, save_path=None, title='Receiver Operating Characteristic (ROC) One vs rest (OVR) Curve Zeroshot Model'):\n",
    "    \"\"\"\n",
    "    Plots ROC curves (One-vs-Rest and micro-average) for a multi-class problem.\n",
    "    \"\"\"\n",
    "    # Binarize the labels for one-vs-rest ROC calculation\n",
    "    y_true_bin = label_binarize(y_true, classes=range(len(class_names)))\n",
    "    n_classes = y_true_bin.shape[1]\n",
    "    \n",
    "    # Compute ROC curve and ROC area for each class\n",
    "    fpr, tpr, roc_auc = {}, {}, {}\n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_true_bin[:, i], y_prob[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "    # Compute micro-average ROC curve and ROC area\n",
    "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_true_bin.ravel(), y_prob.ravel())\n",
    "    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "    # Plot all ROC curves\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    colors = ['aqua', 'darkorange', 'cornflowerblue']\n",
    "    \n",
    "    for i, color in enumerate(colors):\n",
    "        plt.plot(fpr[i], tpr[i], color=color, lw=2,\n",
    "                 label=f'ROC curve of {class_names[i]} vs Rest (area = {roc_auc[i]:0.2f})')\n",
    "\n",
    "    plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "             label=f'micro-average ROC curve (area = {roc_auc[\"micro\"]:.2f})',\n",
    "             color='deeppink', linestyle=':', linewidth=4)\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(title)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300)\n",
    "        \n",
    "    plt.show()\n",
    "\n",
    "    # Print the summary scores\n",
    "    print(\"\\nModel ROC-AUC scores:\")\n",
    "    for i, name in enumerate(class_names):\n",
    "        print(f\"{name} vs rest: {roc_auc[i]:.4f}\")\n",
    "    print(f\"Micro-average: {roc_auc['micro']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision & recall plot Function for Base model and few shot Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision-Recall Curve (PRC) Functions for Base and Fine-tuned Models\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import torch\n",
    "import os\n",
    "\n",
    "def plot_precision_recall_curves(model, X, y, class_names, title=\"Precision-Recall Curve\", save_path=None):\n",
    "    \"\"\"\n",
    "    Plot Precision-Recall curves for a multi-class classification problem.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained PyTorch model\n",
    "        X: Input features (numpy array)\n",
    "        y: True labels (numpy array)\n",
    "        class_names: List of class names\n",
    "        title: Plot title\n",
    "        save_path: Path to save the plot\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with average precision scores\n",
    "    \"\"\"\n",
    "    # Convert to PyTorch tensors if needed\n",
    "    if isinstance(X, np.ndarray):\n",
    "        X_tensor = torch.tensor(X, dtype=torch.float32).to(device)\n",
    "    else:\n",
    "        X_tensor = X.to(device)\n",
    "    \n",
    "    # Get model predictions\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X_tensor)\n",
    "        probas = torch.softmax(outputs, dim=1).cpu().numpy()\n",
    "    \n",
    "    # Binarize the labels for one-vs-rest PR curves\n",
    "    n_classes = len(class_names)\n",
    "    y_bin = label_binarize(y, classes=range(n_classes))\n",
    "    \n",
    "    # If binary classification, reshape y_bin\n",
    "    if n_classes == 2:\n",
    "        y_bin = np.column_stack([1 - y_bin, y_bin])\n",
    "    \n",
    "    # Compute Precision-Recall curve and Average Precision for each class\n",
    "    precision = {}\n",
    "    recall = {}\n",
    "    avg_precision = {}\n",
    "    \n",
    "    for i in range(n_classes):\n",
    "        precision[i], recall[i], _ = precision_recall_curve(y_bin[:, i], probas[:, i])\n",
    "        avg_precision[i] = average_precision_score(y_bin[:, i], probas[:, i])\n",
    "    \n",
    "    # Compute micro-average Precision-Recall curve\n",
    "    precision[\"micro\"], recall[\"micro\"], _ = precision_recall_curve(\n",
    "        y_bin.ravel(), probas.ravel()\n",
    "    )\n",
    "    avg_precision[\"micro\"] = average_precision_score(y_bin, probas, average=\"micro\")\n",
    "    \n",
    "    # Plot Precision-Recall curves\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Colors for each class\n",
    "    colors = ['cyan', 'orange', 'blue', 'red', 'green', 'purple']\n",
    "    \n",
    "    # Plot PR curve for each class\n",
    "    for i, color, name in zip(range(n_classes), colors, class_names):\n",
    "        plt.plot(recall[i], precision[i], color=color, lw=2,\n",
    "                 label=f'{name} (AP = {avg_precision[i]:.3f})')\n",
    "    \n",
    "    # Plot micro-average PR curve\n",
    "    plt.plot(recall[\"micro\"], precision[\"micro\"], color='magenta', linestyle=':', linewidth=4,\n",
    "             label=f'micro-average (AP = {avg_precision[\"micro\"]:.3f})')\n",
    "    \n",
    "    # Add baseline (random classifier)\n",
    "    # For imbalanced datasets, baseline is the proportion of positive class\n",
    "    baseline_precision = np.sum(y_bin, axis=0) / len(y_bin)\n",
    "    for i, (color, name) in enumerate(zip(colors, class_names)):\n",
    "        plt.axhline(y=baseline_precision[i], color=color, linestyle='--', alpha=0.5,\n",
    "                   label=f'{name} baseline (AP = {baseline_precision[i]:.3f})')\n",
    "    \n",
    "    # Set plot properties\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('Recall', fontsize=12)\n",
    "    plt.ylabel('Precision', fontsize=12)\n",
    "    plt.title(title, fontsize=14, fontweight='bold')\n",
    "    plt.legend(loc=\"lower left\", fontsize=10)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add text box with class distribution info\n",
    "    class_counts = np.bincount(y)\n",
    "    total_samples = len(y)\n",
    "    info_text = \"Class Distribution:\\\\n\"\n",
    "    for i, (name, count) in enumerate(zip(class_names, class_counts)):\n",
    "        percentage = (count / total_samples) * 100\n",
    "        info_text += f\"{name}: {count} ({percentage:.1f}%)\\\\n\"\n",
    "    \n",
    "    plt.text(0.02, 0.98, info_text, transform=plt.gca().transAxes, \n",
    "             verticalalignment='top', bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightblue\", alpha=0.8),\n",
    "             fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save figure if path is provided\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Precision-Recall curve saved to: {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # Print average precision scores\n",
    "    print(f\"\\\\n{title} - Average Precision Scores:\")\n",
    "    print(\"=\" * 50)\n",
    "    for i, name in enumerate(class_names):\n",
    "        print(f\"{name:25s}: {avg_precision[i]:.4f}\")\n",
    "    print(f\"{'Micro-average':25s}: {avg_precision['micro']:.4f}\")\n",
    "    \n",
    "    return avg_precision\n",
    "\n",
    "def plot_base_model_prc(model, test_loader, class_names):\n",
    "    \"\"\"\n",
    "    Plot Precision-Recall curves for the base model using DataLoader.\n",
    "    \"\"\"\n",
    "    # Extract data from DataLoader\n",
    "    X_test = []\n",
    "    y_test = []\n",
    "    \n",
    "    for inputs, targets in test_loader:\n",
    "        X_test.append(inputs.numpy())\n",
    "        y_test.extend(targets.numpy())\n",
    "    \n",
    "    X_test = np.vstack(X_test)\n",
    "    y_test = np.array(y_test)\n",
    "    \n",
    "    # Plot PRC\n",
    "    avg_precision = plot_precision_recall_curves(\n",
    "        model, \n",
    "        X_test, \n",
    "        y_test, \n",
    "        class_names,\n",
    "        title=\"Base Model: Precision-Recall Curve\",\n",
    "        save_path=os.path.join(RESULTS_DIR, 'base_model_precision_recall_curve.png')\n",
    "    )\n",
    "    \n",
    "    return avg_precision\n",
    "\n",
    "def plot_fine_tuned_model_prc(model, monomer_X, monomer_y, class_names):\n",
    "    \"\"\"\n",
    "    Plot Precision-Recall curves for the fine-tuned model.\n",
    "    \"\"\"\n",
    "    avg_precision = plot_precision_recall_curves(\n",
    "        model, \n",
    "        monomer_X, \n",
    "        monomer_y, \n",
    "        class_names,\n",
    "        title=\"Few shot Model: Precision-Recall Curve\",\n",
    "        save_path=os.path.join(RESULTS_DIR, 'fine_tuned_model_precision_recall_curve.png')\n",
    "    )\n",
    "    \n",
    "    return avg_precision\n",
    "\n",
    "def compare_prc_curves(base_model, fine_tuned_model, test_X, test_y, monomer_X, monomer_y, class_names):\n",
    "    \"\"\"\n",
    "    Compare Precision-Recall curves between base and fine-tuned models.\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
    "    \n",
    "    # Colors for each class\n",
    "    colors = ['cyan', 'orange', 'blue']\n",
    "    \n",
    "    # Plot base model PRC\n",
    "    base_model.eval()\n",
    "    with torch.no_grad():\n",
    "        X_tensor = torch.tensor(test_X, dtype=torch.float32).to(device)\n",
    "        outputs = base_model(X_tensor)\n",
    "        base_probas = torch.softmax(outputs, dim=1).cpu().numpy()\n",
    "    \n",
    "    y_bin_test = label_binarize(test_y, classes=range(len(class_names)))\n",
    "    \n",
    "    for i, (color, name) in enumerate(zip(colors, class_names)):\n",
    "        precision, recall, _ = precision_recall_curve(y_bin_test[:, i], base_probas[:, i])\n",
    "        avg_precision = average_precision_score(y_bin_test[:, i], base_probas[:, i])\n",
    "        ax1.plot(recall, precision, color=color, lw=2,\n",
    "                label=f'{name} (AP = {avg_precision:.3f})')\n",
    "    \n",
    "    ax1.set_xlim([0.0, 1.0])\n",
    "    ax1.set_ylim([0.0, 1.05])\n",
    "    ax1.set_xlabel('Recall')\n",
    "    ax1.set_ylabel('Precision')\n",
    "    ax1.set_title('Base Model: Precision-Recall Curve')\n",
    "    ax1.legend(loc=\"lower left\")\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot fine-tuned model PRC\n",
    "    fine_tuned_model.eval()\n",
    "    with torch.no_grad():\n",
    "        X_tensor = torch.tensor(monomer_X, dtype=torch.float32).to(device)\n",
    "        outputs = fine_tuned_model(X_tensor)\n",
    "        fine_probas = torch.softmax(outputs, dim=1).cpu().numpy()\n",
    "    \n",
    "    y_bin_monomer = label_binarize(monomer_y, classes=range(len(class_names)))\n",
    "    \n",
    "    for i, (color, name) in enumerate(zip(colors, class_names)):\n",
    "        precision, recall, _ = precision_recall_curve(y_bin_monomer[:, i], fine_probas[:, i])\n",
    "        avg_precision = average_precision_score(y_bin_monomer[:, i], fine_probas[:, i])\n",
    "        ax2.plot(recall, precision, color=color, lw=2,\n",
    "                label=f'{name} (AP = {avg_precision:.3f})')\n",
    "    \n",
    "    ax2.set_xlim([0.0, 1.0])\n",
    "    ax2.set_ylim([0.0, 1.05])\n",
    "    ax2.set_xlabel('Recall')\n",
    "    ax2.set_ylabel('Precision')\n",
    "    ax2.set_title('Fine-tuned Model: Precision-Recall Curve')\n",
    "    ax2.legend(loc=\"lower left\")\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(RESULTS_DIR, 'prc_comparison.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Usage examples:\n",
    "\"\"\"\n",
    "# For base model (after training)\n",
    "class_names = ['Not_Fulfilled', 'Fulfilled', 'Under_Investigation']\n",
    "base_avg_precision = plot_base_model_prc(model, test_loader, class_names)\n",
    "\n",
    "# For fine-tuned model (after cross-validation)\n",
    "fine_tuned_avg_precision = plot_fine_tuned_model_prc(best_model, monomer_X, monomer_y, class_names)\n",
    "\n",
    "# For comparison\n",
    "compare_prc_curves(model, best_model, test_X, test_y, monomer_X, monomer_y, class_names)\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. ZERO-SHOT CROSS-VALIDATION EVALUATION FUNCTION ---\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "def zero_shot_cross_validation(model, X_polymer, y_polymer, n_splits=5):\n",
    "    \"\"\"\n",
    "    Performs 5-fold cross-validation by testing a pre-trained model on each fold.\n",
    "    The model is NOT retrained at any point.\n",
    "    \"\"\"\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=RANDOM_SEED)\n",
    "    summary_metrics = []\n",
    "    \n",
    "    # Store all predictions and targets across all folds for a final confusion matrix\n",
    "    all_fold_preds = np.array([])\n",
    "    all_fold_targets = np.array([])\n",
    "    all_fold_probs = None\n",
    "\n",
    "    print(\"Starting 5-Fold Cross-Validation for Zero-Shot Evaluation\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(skf.split(X_polymer, y_polymer)):\n",
    "        print(f\"\\nFold {fold + 1}/{n_splits}\")\n",
    "        print(\"-\" * 30)\n",
    "        X_test, y_test = X_polymer[test_idx], y_polymer[test_idx]\n",
    "\n",
    "        print(f\"Test samples: {len(X_test)}\")\n",
    "        print(f\"Test distribution: {np.bincount(y_test)}\")\n",
    "\n",
    "        # Create DataLoader for the current test fold\n",
    "        test_dataset_fold = torch.utils.data.TensorDataset(\n",
    "            torch.tensor(X_test, dtype=torch.float32),\n",
    "            torch.tensor(y_test, dtype=torch.long)\n",
    "        )\n",
    "        test_loader_fold = DataLoader(test_dataset_fold, batch_size=32, shuffle=False)\n",
    "\n",
    "        # Evaluate the *same pre-trained model* on this fold\n",
    "        model.eval()\n",
    "        fold_preds, fold_targets, fold_probs = [], [], []\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in test_loader_fold:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = model(inputs)\n",
    "                probs = torch.softmax(outputs, dim=1).cpu().numpy()\n",
    "                preds = np.argmax(probs, axis=1)\n",
    "\n",
    "                fold_preds.extend(preds)\n",
    "                fold_targets.extend(targets.cpu().numpy())\n",
    "                fold_probs.append(probs)\n",
    "        \n",
    "        fold_probs = np.vstack(fold_probs)\n",
    "        \n",
    "        # Store predictions for overall analysis later\n",
    "        all_fold_preds = np.concatenate([all_fold_preds, fold_preds])\n",
    "        all_fold_targets = np.concatenate([all_fold_targets, fold_targets])\n",
    "        if all_fold_probs is None:\n",
    "            all_fold_probs = fold_probs\n",
    "        else:\n",
    "            all_fold_probs = np.vstack([all_fold_probs, fold_probs])\n",
    "\n",
    "        # Calculate metrics for this fold\n",
    "        y_test_bin = label_binarize(y_test, classes=[0,1,2])\n",
    "\n",
    "        fold_report = {\n",
    "            \"accuracy\": accuracy_score(y_test, fold_preds),\n",
    "            \"precision\": precision_score(y_test, fold_preds, average='weighted', zero_division=0),\n",
    "            \"recall\": recall_score(y_test, fold_preds, average='weighted', zero_division=0),\n",
    "            \"f1\": f1_score(y_test, fold_preds, average='weighted', zero_division=0),\n",
    "            \"roc_auc\": roc_auc_score(y_test_bin, fold_probs, average=\"macro\", multi_class=\"ovr\")\n",
    "        }\n",
    "        summary_metrics.append(fold_report)\n",
    "        \n",
    "        print(f\"Accuracy:  {fold_report['accuracy']:.4f}\")\n",
    "        print(f\"F1 Score:  {fold_report['f1']:.4f}\")\n",
    "        print(f\"ROC-AUC:   {fold_report['roc_auc']:.4f}\")\n",
    "\n",
    "    return summary_metrics, all_fold_targets, all_fold_preds, all_fold_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data: Divide the 96 samples to get the 20-sample X_test.\n",
    "\n",
    "Load Model: Load the pre-trained base_model.\n",
    "\n",
    "Evaluate: Test the base_model directly on the 20 samples in X_test.\n",
    "\n",
    "Report: Show the final results (Confusion Matrix, ROC Curve, etc.) from that single evaluation.\n",
    "\n",
    " Data has been split into three distinct sets:\n",
    " - Fine-Tuning Set: 20 samples (IGNORED for Zero-Shot)\n",
    " - Cross-Validation Set: 56 samples (IGNORED for Zero-Shot)\n",
    " - Noble Test Set (Final Evaluation): 20 samples (USED for Zero-Shot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5. UNIVERSAL DATA SPLITTING ---\n",
    "\n",
    "# Load the polymer data\n",
    "df_polymer = pd.read_csv(POLYMER_DATA_PATH)\n",
    "\n",
    "# Create the PyTorch Dataset\n",
    "property_cols = [col for col in df_polymer.columns if col.startswith('Property_')]\n",
    "polymer_dataset = PolymerToxicityDataset(df_polymer, 'fingerprints', property_cols, 'Hazard_Criteria_encoded')\n",
    "sample_features, _ = polymer_dataset[0]\n",
    "input_dim = sample_features.shape[0]\n",
    "\n",
    "# First, prepare the full X and y NumPy arrays\n",
    "X_polymer = np.array([data[0].numpy() for data in polymer_dataset])\n",
    "y_polymer = np.array([data[1].numpy() for data in polymer_dataset])\n",
    "\n",
    "# Define the sizes for each set based on the new strategy\n",
    "n_total = len(X_polymer)\n",
    "n_test = 20      # 20 samples for the final noble test set\n",
    "n_finetune = 20  # 20 samples for the initial fine-tuning\n",
    "\n",
    "# Step 1: Split off the \"Noble Test Set\"\n",
    "X_train_pool, X_test, y_train_pool, y_test = train_test_split(\n",
    "    X_polymer,\n",
    "    y_polymer,\n",
    "    test_size=n_test,\n",
    "    random_state=RANDOM_SEED,\n",
    "    stratify=y_polymer\n",
    ")\n",
    "\n",
    "# Step 2: Split the training pool into a \"Fine-Tuning Set\" and a \"Cross-Validation Set\"\n",
    "# NOTE: We create these so the data is split identically to the other notebooks, but we won't use them here.\n",
    "X_cv, X_finetune, y_cv, y_finetune = train_test_split(\n",
    "    X_train_pool,\n",
    "    y_train_pool,\n",
    "    test_size=n_finetune,\n",
    "    random_state=RANDOM_SEED,\n",
    "    stratify=y_train_pool\n",
    ")\n",
    "\n",
    "# --- Verification ---\n",
    "print(\"Data has been split into three distinct sets:\")\n",
    "print(f\" - Fine-Tuning Set: {len(X_finetune)} samples (IGNORED for Zero-Shot)\")\n",
    "print(f\" - Cross-Validation Set: {len(X_cv)} samples (USED  for CV Zero-Shot)\")\n",
    "print(f\" - Noble Test Set (Final Evaluation): {len(X_test)} samples (USED for Zero-Shot)\")\n",
    "\n",
    "# --- 6. LOAD MODEL AND EVALUATE ON NOBLE TEST SET ---\n",
    "\n",
    "# Load Pre-Trained Model\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Loading Pre-trained Base Model...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load hyperparameters to build the model architecture\n",
    "with open(PARAMS_PATH, 'r') as f:\n",
    "    best_params = json.load(f)\n",
    "\n",
    "# Instantiate the model with the loaded architecture\n",
    "base_model = create_enhanced_model_with_best_params(best_params, input_dim)\n",
    "\n",
    "# Load the pre-trained weights\n",
    "base_model.load_state_dict(torch.load(BASE_MODEL_PATH, map_location=device))\n",
    "base_model.to(device)\n",
    "print(\"Pre-trained base model loaded successfully.\\n\")\n",
    "\n",
    "# Final Zero-Shot Evaluation on the Noble Test Set\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PERFORMING ZERO-SHOT EVALUATION ON THE NOBLE TEST SET (20 SAMPLES)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create a DataLoader for the noble test set\n",
    "test_dataset_noble = torch.utils.data.TensorDataset(\n",
    "    torch.tensor(X_test, dtype=torch.float32), \n",
    "    torch.tensor(y_test, dtype=torch.long)\n",
    ")\n",
    "test_loader_noble = DataLoader(test_dataset_noble, batch_size=best_params['batch_size'], shuffle=False)\n",
    "\n",
    "# Evaluate the final model on the noble test set\n",
    "final_metrics = evaluate_model(base_model, test_loader_noble, class_names)\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\nDetailed Classification Report for Zero-Shot Model:\")\n",
    "print(\"-\" * 60)\n",
    "print(classification_report(final_metrics['targets'], final_metrics['predictions'], target_names=class_names))\n",
    "\n",
    "# Confusion Matrix\n",
    "print(\"\\nConfusion Matrix for Zero-Shot Model:\")\n",
    "cm_zero_shot = confusion_matrix(final_metrics['targets'], final_metrics['predictions'])\n",
    "plot_confusion_matrix(\n",
    "    cm_zero_shot, \n",
    "    class_names, \n",
    "    save_path=os.path.join(RESULTS_DIR, 'zeroshot_model_confusion_matrix.png')\n",
    ")\n",
    "\n",
    "# Per-class Accuracy\n",
    "print(\"\\nPer-class Accuracy for Zero-Shot Model:\")\n",
    "print(\"-\" * 40)\n",
    "for i, name in enumerate(class_names):\n",
    "    true_samples_for_class = cm[i].sum()\n",
    "    if true_samples_for_class > 0:\n",
    "        class_accuracy = cm[i, i] / true_samples_for_class\n",
    "        print(f\"{name:25s}: {class_accuracy:.4f} ({cm[i, i]}/{true_samples_for_class})\")\n",
    "    else:\n",
    "        print(f\"{name:25s}: No test samples in this class\")\n",
    "overall_accuracy = np.trace(cm) / np.sum(cm)\n",
    "print(f\"\\nOverall Accuracy: {overall_accuracy:.4f}\")\n",
    "\n",
    "# ROC AUC Curve\n",
    "print(\"\\nROC AUC Curve (One vs Rest) for Zero-Shot Model:\")\n",
    "plot_roc_curves(final_metrics['targets'], final_metrics['probabilities'], class_names, save_path=os.path.join(RESULTS_DIR, \"zeroshot_final_roc_curve.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-Fold Cross-Validation on the CV Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load Pre-Trained Model ---\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Loading Pre-trained Base Model...\")\n",
    "with open(PARAMS_PATH, 'r') as f:\n",
    "    best_params = json.load(f)\n",
    "base_model = create_enhanced_model_with_best_params(best_params, input_dim)\n",
    "base_model.load_state_dict(torch.load(BASE_MODEL_PATH, map_location=device))\n",
    "base_model.to(device)\n",
    "print(\"Pre-trained base model loaded successfully.\\n\")\n",
    "\n",
    "# --- 5-Fold Cross-Validation on the CV Set (56 samples) ---\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_SEED)\n",
    "summary_metrics = []\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Starting 5-Fold Cross-Validation on the CV Set\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for fold, (_, test_idx) in enumerate(skf.split(X_cv, y_cv)):\n",
    "    print(f\"\\nFold {fold + 1}/5\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    X_test_fold, y_test_fold = X_cv[test_idx], y_cv[test_idx]\n",
    "    \n",
    "    print(f\"Test samples for this fold: {len(X_test_fold)}\")\n",
    "    \n",
    "    test_dataset_fold = torch.utils.data.TensorDataset(torch.tensor(X_test_fold, dtype=torch.float32), torch.tensor(y_test_fold, dtype=torch.long))\n",
    "    test_loader_fold = DataLoader(test_dataset_fold, batch_size=best_params['batch_size'], shuffle=False)\n",
    "    \n",
    "    # Evaluate the base model on this fold's test set\n",
    "    fold_metrics = evaluate_model(base_model, test_loader_fold, class_names)\n",
    "    \n",
    "    print(f\"Accuracy:  {fold_metrics['accuracy']:.4f}\")\n",
    "    print(f\"F1 Score:  {fold_metrics['f1']:.4f}\")\n",
    "    print(f\"ROC-AUC:   {fold_metrics['roc_auc']:.4f}\")\n",
    "    summary_metrics.append(fold_metrics)\n",
    "    \n",
    "# --- Summary statistics from the 5-Fold CV on the development set ---\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ZERO-SHOT ROBUSTNESS: SUMMARY STATISTICS ACROSS CV FOLDS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Extract each metric from the list of dictionaries\n",
    "accuracies = [m['accuracy'] for m in summary_metrics]\n",
    "precisions = [m['precision'] for m in summary_metrics]\n",
    "recalls = [m['recall'] for m in summary_metrics]\n",
    "f1s = [m['f1'] for m in summary_metrics]\n",
    "roc_aucs = [m['roc_auc'] for m in summary_metrics]\n",
    "\n",
    "# Calculate and print the stats\n",
    "print(f\"ACCURACY  : {np.mean(accuracies):.4f} ¬± {np.std(accuracies):.4f} (min: {np.min(accuracies):.4f}, max: {np.max(accuracies):.4f})\")\n",
    "print(f\"PRECISION : {np.mean(precisions):.4f} ¬± {np.std(precisions):.4f} (min: {np.min(precisions):.4f}, max: {np.max(precisions):.4f})\")\n",
    "print(f\"RECALL    : {np.mean(recalls):.4f} ¬± {np.std(recalls):.4f} (min: {np.min(recalls):.4f}, max: {np.max(recalls):.4f})\")\n",
    "print(f\"F1        : {np.mean(f1s):.4f} ¬± {np.std(f1s):.4f} (min: {np.min(f1s):.4f}, max: {np.max(f1s):.4f})\")\n",
    "print(f\"ROC_AUC   : {np.mean(roc_aucs):.4f} ¬± {np.std(roc_aucs):.4f} (min: {np.min(roc_aucs):.4f}, max: {np.max(roc_aucs):.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary and Conclusions\n",
    "\n",
    "This enhanced notebook has successfully implemented:\n",
    "\n",
    "### Key Enhancements:\n",
    "1. **PReLU Activation Function**: Parametric ReLU with learnable negative slope for better gradient flow\n",
    "2. **Early Stopping**: Prevents overfitting with patience-based stopping and weight restoration\n",
    "3. **Multiple Activation Functions**: Support for ReLU, PReLU, SiLU, LeakyReLU, ELU, and GELU in Optuna optimization\n",
    "4. **Comprehensive 5-Fold Cross-Validation**: Detailed metrics, classification reports, and statistical analysis\n",
    "5. **Data Distribution Visualization**: Bar plots for both Tox21 and monomer datasets\n",
    "6. **Enhanced ROC Curve Analysis**: Separate functions for DataLoader and raw data inputs\n",
    "7. **Advanced Optuna Optimization**: Improved pruning, visualization, and parameter importance analysis\n",
    "\n",
    "### Results:\n",
    "- The transfer learning approach successfully leverages the Tox21 dataset for polymer toxicity prediction\n",
    "- PReLU activation and early stopping help improve model performance and prevent overfitting\n",
    "- Comprehensive cross-validation provides robust evaluation of the few-shot learning approach\n",
    "- Detailed visualizations and reports enable thorough analysis of model performance\n",
    "\n",
    "### Next Steps:\n",
    "- Consider ensemble methods for further performance improvements\n",
    "- Explore additional data augmentation techniques for the limited polymer dataset\n",
    "- Implement model interpretability methods to understand feature importance\n",
    "- Consider focal loss or other advanced techniques for handling class imbalance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
